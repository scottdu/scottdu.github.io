<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="多读书 多看报 少吃零食 多睡觉">
<meta property="og:type" content="website">
<meta property="og:title" content="阿杜S考特">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="阿杜S考特">
<meta property="og:description" content="多读书 多看报 少吃零食 多睡觉">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="阿杜S考特">
<meta name="twitter:description" content="多读书 多看报 少吃零食 多睡觉">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>阿杜S考特</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">阿杜S考特</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">个人空间</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/15/math1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/math1/" itemprop="url">某初中数学题的解法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-15T11:30:49+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>已知$x_1+x_2=1$，$x_1x_2=-1$，求$x_1^7+x_2^7=?$</p>
<p>这是小朋友问我的一个题目，第一感觉就是用求根公式解方程组，分别求出$x_1$和$x_2$的解，然后代入$x_1^7+x_2^7$，然后解决完毕，呵呵！</p>
<h2 id="解法一：粗暴解法"><a href="#解法一：粗暴解法" class="headerlink" title="解法一：粗暴解法"></a>解法一：粗暴解法</h2><p>先给出一个定理: 设一元二次方程$ax^2+bx+c=0(a,b,c\in R, a \ne 0)$的两个根$x_1$和$x_2$,<br>由求根公式可以得到</p>
<script type="math/tex; mode=display">x_{1,2}=\frac{-b \pm \sqrt{b^2-4ac}}{2a}</script><p>根据条件可以得到$x_1$和$x_2$是一元二次方程$x^2-x-1=0$的两个根。由一元二次方程的求根公式可知</p>
<script type="math/tex; mode=display">x_{1,2}=\frac{1 \pm \sqrt 5}{2}</script><p>然后</p>
<script type="math/tex; mode=display">x_1^7+x_2^7=(\frac{1+\sqrt5}{2})^7+(\frac{1-\sqrt5}{2})^7 = 29</script><p>问题如果用计算器计算无理数，可能会丢失精度，更何况没有计算器，对于这种无理数的高次幂运算需要用到二项式定理，太费<code>CPU</code>了，这不是正解。</p>
<h2 id="解法二：迭代法"><a href="#解法二：迭代法" class="headerlink" title="解法二：迭代法"></a>解法二：迭代法</h2><p>先求解根的低次幂和</p>
<p>$s_1=x_1+x_2=1$</p>
<p>$s_2=x_1^2+x_2^2=(x_1+x_2)^2-2x_1x_2=3$</p>
<p>$s_3=x_1^3+x_2^3=(x_1+x_2)^3-3x_1x_2(x_1+x_2)=4$</p>
<p>$s_4=x_1^4+x_2^4=(x_1^2+x_2^2)^2-2(x_1^2x_2^2)^2=7$</p>
<p>然后惊奇地发现$s_2=s_1+0,s3=s1+s2,s4=s3+s2$,这不是斐波那契额数列<code>Fibonacci sequence</code>的特性吗？</p>
<p>于是就直接得出</p>
<p>$s_5=s_4+s_3=11$</p>
<p>$s_6=s_5+s_4=18$</p>
<p>$s_7=s_6+s_5=29$</p>
<p>所以$x_1^7+x_2^7=29$，解决完毕，但是这毕竟是猜想，结果虽然是正确的，但是不严谨！而且如果题目的条件变化了，是不是都满足<code>Fibonacci sequence</code>的特性呢？由此引出了这样的问题：如果我们知道了两个未知数的和以及乘积，是否可以知道所有根的任意次幂的和呢？是不是可以更加泛化一下呢？</p>
<h2 id="泛化问题"><a href="#泛化问题" class="headerlink" title="泛化问题"></a>泛化问题</h2><p>已知$x_1+x_2=a$，$x_1x_2=b$，<br>求$x_1^n+x_2^n=? 其中a,b都是常数，n \in N$</p>
<p>解：</p>
<p>令$s_n=x_1^n+x_2^n$，则</p>
<p>$s_n=x_1^{n-1}x_1+x_2^{n-1}x_2$</p>
<p>$s_n=x_1^{n-1}(a-x_2)+x_2^{n-1}(a-x_1)$</p>
<p>$s_n=ax_1^{n-1}-x_1^{n-1}x_2+ax_2^{n-1}-x_1x_2^{n-1}$</p>
<p>$s_n=a(x_1^{n-1}+x_2^{n-1})-x_1x_2(x_1^{n-2}+x_2^{n-2})$</p>
<p>$s_n=a(x_1^{n-1}+x_2^{n-1})-b(x_1^{n-2}+x_2^{n-2})$</p>
<p>于是得到了以下的递推公式:</p>
<script type="math/tex; mode=display">s_n=as_{n-1}-bs_{n-2}</script><p>那么对于原始问题已知$a=1,b=-1$<br>可以得到<script type="math/tex">s_n=s_{n-1}+s_{n-2}</script><br>由此验证了解法二的严谨性。</p>
<h2 id="延拓新的问题"><a href="#延拓新的问题" class="headerlink" title="延拓新的问题"></a>延拓新的问题</h2><p>如果$x_1,x_2$没有实数解，以上方法是否适用?</p>
<p>看一个例子,对于泛化的公式,如果$a=b=2$，<br>即</p>
<script type="math/tex; mode=display">s_1=x_1+x_2=2, x_1x_2=2</script><p>显然</p>
<script type="math/tex; mode=display">x_{1,2}=1 \pm i</script><p>那么</p>
<p>$s_2=x_1^2+x_2^2=(1+i)^2+(1-i)^2=0$</p>
<p>按照递推公式</p>
<p>$s_3=2s_2-2s_1=-4$</p>
<p>$s_4=2s_3-2s_2=-8$</p>
<p>$s_5=2s_4-2s_3=-8$</p>
<p>$s_6=2s_5-2s_4=0$</p>
<p>$s_7=2s_6-2s_5=16$</p>
<p>$s_8=2s_7-2s_6=32$</p>
<p>$s_9=2s_8-2s_7=32$</p>
<p>$s_{10}=2s_9-2s_8=0$</p>
<p>貌似也ok，待证明</p>
<h2 id="总结一下泛华问题"><a href="#总结一下泛华问题" class="headerlink" title="总结一下泛华问题"></a>总结一下泛华问题</h2><p>对于已知$x_1+x_2=a$，$x_1x_2=b$，<br>求$s_n =x_1^n+x_2^n=? 其中a,b都是常数，n \in N$</p>
<p>就会有</p>
<script type="math/tex; mode=display">s_n=as_{n-1}-bs_{n-2}, 其中s_0=2,s_1=a</script><p>最后用<code>python</code>代码实现一下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a是和, b是积, n是幂</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span><span class="params">(a,b,n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">0</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line">    <span class="keyword">return</span> a*solution(a,b,n<span class="number">-1</span>)- b*solution(a,b,n<span class="number">-2</span>)</span><br></pre></td></tr></table></figure>
<p>运行结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(solution(1,-1,0))</span><br><span class="line">2</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(solution(1,-1,1))</span><br><span class="line">1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(solution(1,-1,2))</span><br><span class="line">3</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(solution(1,-1,3))</span><br><span class="line">4</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(solution(1,-1,4))</span><br><span class="line">7</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(solution(1,-1,5))</span><br><span class="line">11</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(solution(1,-1,6))</span><br><span class="line">18</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(solution(1,-1,7))</span><br><span class="line">29</span><br></pre></td></tr></table></figure>
<p>然后</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; print(solution(1,-1,100))</span><br><span class="line"># 等了好久都没算出来</span><br><span class="line"># 容易理解的算法性能差，性能好的算法不容易理解，这就是生活！！！！</span><br></pre></td></tr></table></figure>
<p>经典优化方案，空间换时间:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span><span class="params">(a,b,n)</span>:</span></span><br><span class="line">    sn,s0,s1 = <span class="number">0</span>, <span class="number">2</span>, a</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        sn = a*s1 - b*s0</span><br><span class="line">        s0 = s1</span><br><span class="line">        s1 = sn</span><br><span class="line">    <span class="keyword">return</span> sn</span><br></pre></td></tr></table></figure>
<p>测试100</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(solution(<span class="number">1</span>,<span class="number">-1</span>,<span class="number">100</span>))</span><br><span class="line"><span class="number">792070839848372253127</span></span><br><span class="line"><span class="comment"># 计算结果秒出了</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/06/education1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/06/education1/" itemprop="url">面向未来的教育理念及方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-06T15:39:04+08:00">
                2019-01-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="几个问题"><a href="#几个问题" class="headerlink" title="几个问题"></a>几个问题</h2><ul>
<li>幼儿园虐童事件的根源性问题分析</li>
<li>未来社会需要什么样的人才？</li>
<li>培养面向未来的人才，应该采取怎样的教育理念和方法？</li>
</ul>
<h2 id="关于虐童的问题"><a href="#关于虐童的问题" class="headerlink" title="关于虐童的问题"></a>关于虐童的问题</h2><p>大家一谈到虐童，就会非常愤怒，然后骂出现了这样的问题的机构丧尽天良，或者呼吁在幼儿园里无死角地装上摄像头。问题是，这样就可以避免虐童了吗？</p>
<p>其实这里最大的问题是，为什么会有幼教虐待孩子？按理说，面对那么可爱、那么无助的孩子，普通人根本下不去手，能够做出这样的事情的人，内心一定是出了问题。这里面可能有这些人家庭的因素，但也有社会和教育应该承担的部分。也就是说，当我们去追问他们作为教育者合不合格的时候，其实他们早就已经成为了教育的牺牲品。这里的教育，不光指幼儿园，还有包括小学、中学、大学在内的整体系统。</p>
<p>教育要保证的，是孩子们在心灵健康成长的基础上对社会的适应，进而能对社会做出贡献。可是我们的教育，只注重用一个可以衡量的方式把孩子分成三六九等，然后送进不同的小学、中学、大学。为了这样的教育目的，凡是能让人拉开差距的就会被重视，哪里会关注培养孩子情操的学问和面向未来的技能呢？</p>
<p>我对曾经的一个电视画面印象深刻：整个班的高三学生，一边打着吊瓶一边备战高考。这样的孩子，分明在被培养为学习机器，哪里会有丝毫顾及到他们的心灵。不客气地说，这样培养出来的孩子，也许在我们现有的教育制度下能够得到高分，但他们的心灵一定是伤痕累累的。这样的孩子如果哪天做出了什么出格的举动，我一点不会惊讶。</p>
<p>孙瑞雪教育机构的创始人孙瑞雪老师，20年前就在国内办幼儿园，可是当周围兴起了越来越多的连锁幼儿园、这些连锁的规模越来越大的时候，曾经有人问她：您怎么不赶快搞连锁、赶快上市？她感慨到：找不到那么多合格的幼教老师啊！幼师刚毕业的孩子，自己尚且带着很多心灵的伤痛和问题，根本不知道怎么爱孩子，她们和孩子的互动对孩子的成长都是不利的，所以，首先需要关注她们内心的成长，给她们做心理辅导，让她们的内心慢慢健全起来，她们才能成为合格的幼教老师，这个周期是以年来计算的。所以怎么可能快速扩张呢？</p>
<p>所以大家就知道现在的问题了：<br>一方面找不到足够多的合格的幼教老师；<br>一方面在商业的诱惑下盲目扩张，不出问题才怪。<br>即使没有出现那样明显的虐童行为的幼儿园也不要庆幸，孩子是脆弱而敏感的，不能全身心地爱孩子的老师，很容易把自己的情绪发泄到孩子身上，也许一次恶语相向、一次冷暴力、甚至一个冰冷的眼神，都会让孩子们受到伤害。</p>
<p>回到今天的主题，我们希望，借着现在社会上的问题，我们不只是就事论事要求赔偿、要求处理，而是更加深入地检讨一下我们教育当中出现的各种问题。</p>
<p>大家都知道，普通学生从小学一直到高中毕业，除了放假，每天都在重复同样的学习生活。每个人学的内容都一样，每学期也都有固定的期中、期末考试。这种教育模式，大致上有这么五个特点：</p>
<p>第一，所有学生的学习时间和学习课程都是统一的，而且不同届的学生学的东西其实都差不多，所有的问题尽量要有标准答案，这样有利于老师做评判。</p>
<p>第二，学习效果的评价主要就是通过考试来看对课程的掌握情况，最多是满分，错了就扣分，不是奖励优秀而是惩罚错误，成绩好的学生会获得更好的教育资源，造成孩子对老师、对正确答案的顺从，而进入社会之后你就会发现，哪里有什么正确答案。</p>
<p>第三，老师上课大多是 “填鸭式”教学，和学生的交流互通比较少，课堂本质上由教师主导，甚至有意无意地强调老师的权威性，学生们的参与度比较低，学生意见不受重视。</p>
<p>第四，学生每天的作业一般都是第二天才能得到反馈，这就意味着他们无法及时判断自己的学习情况，无法自己作出调整。</p>
<p>第五，绝大多数学校都是鼓励竞争而不是鼓励合作。</p>
<p>对于这种教育模式，英国教育学家 <code>Ken Robinson</code> 曾经在 <code>TED</code> 发表过一个非常著名的演讲，叫“学校扼杀创造力吗？”，这是目前 <code>TED</code> 最受欢迎的演讲之一。他说，学校和家长，很可能正在不知不觉地扼杀孩子的潜力。因为每个孩子的天赋和潜力都是不同的，而我们却总想用批量生产的方式造出一个个所谓的高材生。注意，这里不光扼杀了孩子的潜力，也扼杀了心灵。</p>
<p>随着社会经济的发展，尤其是人工智能时代的到来，如何让孩子进入社会后具有竞争力，已经是亟待解决的问题了，很明显，我们应该改变这样的教育模式。那在我们讨论如何改变之前，就得先了解一下未来的社会需要什么样的人才。关于这个问题，结合未来社会发展的趋势来分析，未来的人才会有四个大趋势：</p>
<p>第一，科学技术正在彻底改变我们的社会结构，大量重复性的劳动一定会被机器取代，那些懂得使用技术的人会被极大地赋能。所以，未来的人才必须要掌握运用科技的能力。这需要的不是会解题的机器，而是具有完整人格的个体。</p>
<p>第二，社会分工越来越复杂化，每个人都将拥有多重分工和多重身份。比如你可以同时是程序员、厨师和小说家。因此，未来的人才要有跨学科快速学习能力。</p>
<p>第三，社会分工复杂，导致协同成本提高，因此，未来人才需要强大的协作和沟通能力，需要心灵的健全和成熟，需要对他人和社会的理解和关注。</p>
<p>第四，社会快速发展将会带来很多以前不存在的新问题，所以，解决问题的能力也是必不可少的，这需要不怕面对问题、甚至希望面对和解决更复杂问题的自信。</p>
<p>总的来说，未来的优秀人才，需要拥有强大的跨学科快速学习能力和沟通协作能力，并且能够运用科技等最新手段为社会解决问题。这样的人将会主导未来社会，也将是我们社会发展的动力和希望。如果我们想通过教育培养出这样的高素质人才，那就应该重新思考教育的本质和教育方法了。</p>
<p>今天，我将从三个方面为你描述一下培养面向未来的人才应该采取怎样的教育理念和方法。</p>
<h2 id="一、学什么？"><a href="#一、学什么？" class="headerlink" title="一、学什么？"></a>一、学什么？</h2><p>首先，为了培养未来人才，我们需要重新定义学习内容。过去，我们学习的重点是按知识划分的不同学科，比如语文、数学、英语等等。而现在，我们应该重点学习具体的能力，比如批判性思维能力、表达能力、逻辑能力等等。因为，知识是会不断更替的，我们现在要做知识领域的游牧民族，但让孩子们掌握了学习的技能、解决问题的方法，他们就会不断地自我迭代、自我进化，而且在解决问题的过程中、在和同学和环境的互动中，他们的内心也得以成长，建立起对社会和对环境的自信。</p>
<p>20世纪50年代，美国科学教育学者提出了<code>STEM</code>教育的概念，这个概念在之后的几十年，经过不断完善，最终形成了<code>STEAM</code>教育，<code>S-T-E-A-M</code>五个英文字母分别代表 <code>Science</code> 科学，<code>Technology</code> 技术，<code>Engineering</code> 工程，<code>Art</code> 艺术以及 <code>Mathematics</code> 数学。它们并不是简单地整合原来的分科体系，而是创新地代表了五种思维模式，它们分别是：</p>
<p>科学代表“假设-验证”的思维模式；</p>
<p>技术代表“如何应用”的思维模式；</p>
<p>工程代表“系统性思考”的思维模式；</p>
<p>艺术代表“感性化理解”的思维模式；</p>
<p>数学代表“第一性原理”的思维模式。</p>
<p>举个例子，假设目前存在物流效率低下的问题。那么，对于这个问题，经过 <code>STEAM</code> 训练的学生会这样思考：</p>
<p>首先他会问自己“物流效率低的原因可能有哪些？”<br>接着他会开始发散思考，有可能是“城市交通拥堵”或者是“路径规划不合理”。</p>
<p>然后，他会分别针对这两个原因思考解决方案。比如，如果是“城市交通拥堵”的问题，运用“系统性思维”他会想到：假设物流系统能有专用的路线，就可以提高物流效率。当然，物流效率低的原因有可能只是快递员工作不上心，这就要求学生得具备“感性化理解”的思维模式。</p>
<p>这就是 <code>STEAM</code> 教育的目的，给孩子们一套完整的思考和解决问题的思路。</p>
<p>但要注意的是，上面我们说得好像挺轻巧，但真的想要运用 <code>STEAM</code> 理念进行教学，就得重新变革教学与学习的模式，而这种转变，需要最新的科技来帮助实现。</p>
<h2 id="二、怎么学？"><a href="#二、怎么学？" class="headerlink" title="二、怎么学？"></a>二、怎么学？</h2><p>重新定义了“学什么”的问题后，下一步就是要看怎么学了。</p>
<p>华东师范大学的袁振国教授在他的著作《课堂的革命》里，描述了传统课堂里老师和学生的关系：老师是主动的支配者，学生是被动的服从者。老师和学生就是管教和被管教的关系。师生之间不能平等交流意见，甚至不能平等地探讨学科知识。而未来的教育需要彻底改变这个局面，因为学生才是课堂的主体。所以，想办法把科技运用到课堂中，让学生做课堂的主人，老师只起辅助作用，才是未来我们需要的教育方式。</p>
<p>举个例子，相信你对虚拟现实技术已经有不少了解了，你想过没有，如果在教学过程中用上虚拟现实，一定能颠覆学生的课堂体验，提高学生和知识的互动性，真正让学生在体验中学习。现在，在这个领域已经出现先行者了。<code>ClassVR</code>就是一家帮助课堂使用<code>VR</code>进行教学的一站式解决方案提供商。它隶属于著名教学设备公司<code>Avantis</code>。现在已经有好几个国家的机构跟它合作了，其中也包括中国。<code>ClassVR</code>不光提供虚拟现实眼镜，还为老师上课提供了丰富的教学场景资料库，它能让学生更直观地体验学习内容。比如，在一堂生物课上，学生戴上<code>VR</code>眼镜后就能看到一个三维的心脏模型，他们可以通过手指操作让心脏模型旋转、扩大以及深入地观察心脏内部。而且，当点击模型的某个具体部分时，界面上还会显示它的功能和结构，以及其他的辅助教学材料。这样学生就能按照自己的喜好独立学习，老师也从内容输出者变成了答疑者。这样一来，学生更深入地理解了知识，他们的记忆也就更牢固、更有层次。这种方式还能激励学生自主学习，锻炼他们的综合能力。所以我们说，科技正在促进教育方式的变革，让我们更好地培养未来社会的高素质人才。</p>
<h2 id="三、和谁学？"><a href="#三、和谁学？" class="headerlink" title="三、和谁学？"></a>三、和谁学？</h2><p>关于未来的教育问题，除了“学什么”“怎么学”以外，“跟谁学”也很关键。因为高水平的老师总是稀缺的，在中国，优质的教学资源一直在向城市聚拢，而乡镇村里的教育水平却比较差，这就造成了“教育公平”的大问题，要培养出更多的人才，这个问题必须得到良好的解决。解决教育不平衡的重要方法，是利用名师资源，让名师影响更多的学生。著名的经济学家和教育家汤敏老师，曾经倡导过“双师制”，简单来说， “双师制”就是运用互联网，让偏远乡村的孩子也能和城市里的孩子一样享受优质的教学资源。可以说，运用互联网，我们最终可以将这个问题彻底解决。其实很早就有人提出过把互联网运用到教育领域。2001年<code>MIT</code>发布 <code>OpenCourseWare</code>平台以后，越来越多的网络教学网站就开始出现了。它们有一个统称，叫“慕课”。随后引起了一波慕课风潮。国外比较著名的慕课网站有斯坦福大学的<code>Coursera</code>，哈佛大学的<code>edX</code>以及硅谷创业公司开发的<code>Udacity</code>平台，而国内慕课的代表有网易公开课和果壳慕课项目。这些慕课并不是简单地播放名师教学视频，而是把一堂课的内容分解成若干知识点，每节课由十几分钟的短视频组成。另外，慕课还设计了随堂考试模式。当你学完一个知识点，需要全部答对跳出的问题后才能继续学下一堂课。比如程序设计教学网站<code>Codecademy</code>，这个网站上的课程采用了“概念+练习”的方式，也就是先说明一个概念，再辅助练习。这些练习都具有确定的目标，而且你完成后就能马上获得结果。也就是说，你可以得到学习成果的“及时反馈”。开篇我们就说，现在的教育缺乏“及时反馈”，所以学生很难实时评估自己的学习情况，而互联网则帮助我们直接在课堂上做到了这一点。汤敏老师的专著《慕课革命——互联网如何改变教育》对慕课有更详尽、更深入的介绍。慕课还有个创新，就是开创了“翻转课堂模式”，也就是学生在家里通过慕课听课，然后在学校里讨论，并由老师答疑。著名的可汗学院也采用了类似的想法。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>第一，传统的应试教育已经无法满足现代社会快速发展对人才的需求了。我们需要培养面向未来的高素质人才，这要求我们必须转变对教育的看法，教育绝不是批量化生产的“机器”，而是培养具有完整人格个体的“生态系统”。</p>
<p>第二，把科技运用到课堂当中可以极大改善教学质量，让学生能够更直观地体验学习内容，更深入地掌握知识，提高对知识的兴趣度。可以说，科技让课堂的“权力”发生了反转，学生拥有了自主学习的自由，老师从知识的传授者变成了答疑者。</p>
<p>第三，教育资源的不公平是需要解决的一大难题，因为优质教育资源总会自然地向头部聚集。但随着互联网成为社会发展的底层工具，让我们有机会创建一个“共享技能”的时代，可以尽可能地抹平教育的不平衡现状。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/26/tensorflow-notebook/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/26/tensorflow-notebook/" itemprop="url">tensorflow学习笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-26T00:45:14+08:00">
                2018-07-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="搭建一个神经网络-总结搭建方法"><a href="#搭建一个神经网络-总结搭建方法" class="headerlink" title="搭建一个神经网络,总结搭建方法"></a>搭建一个神经网络,总结搭建方法</h2><ul>
<li><p>基于Tensorflow的NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重（参数），得到模型。</p>
</li>
<li><p>张量(tensor)：多维数组（列表） 阶：张量的维数</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>维数</th>
<th>阶</th>
<th>名字</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>0-D</td>
<td>0</td>
<td>标量 scalar</td>
<td>s=123</td>
</tr>
<tr>
<td>1-D</td>
<td>1</td>
<td>向量 vector</td>
<td>v=[1,2,3]</td>
</tr>
<tr>
<td>2-D</td>
<td>2</td>
<td>矩阵 matrix</td>
<td>m=[[1,2,3],[4,5,6],[7,8,9]]</td>
</tr>
<tr>
<td>n-D</td>
<td>n</td>
<td>张量 tensor</td>
<td>t=[[[… （n个中括号）</td>
</tr>
</tbody>
</table>
</div>
<p>张量可以表示0阶到n阶数组(列表)</p>
<ul>
<li>数据类型：<code>tf.float32</code> <code>tf.int32</code> …</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line">result = a+b</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>显示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;add:0&quot;, shape=(2,), dtype=float.32)</span><br></pre></td></tr></table></figure>
<p>其中<code>add:</code>表示<code>节点名</code>，<code>0</code>表示<code>第0个输出</code>,<code>shape</code>表示<code>维度</code></p>
<p>注意vim</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.vimrc写入：</span><br><span class="line">set ts=4    # tab键转成4个空格</span><br><span class="line">set nu      # 显示行号</span><br></pre></td></tr></table></figure>
<ul>
<li>计算图(Graph)：搭建神经网络的计算过程，只搭建，不运算。</li>
</ul>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-17-15318034642927.jpg" width="200"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>]])</span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>],[<span class="number">4.0</span>]])</span><br><span class="line"></span><br><span class="line">y = tf.matmul(x,w)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>显示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"matmul:0"</span>,shape(<span class="number">1</span>,<span class="number">1</span>),dtype=float32)</span><br></pre></td></tr></table></figure>
<ul>
<li>会话(Session):执行计算图中的节点运算。</li>
</ul>
<p>计算<code>1.0*3.0+2.0*4.0=11.0</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>]])</span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>],[<span class="number">4.0</span>]])</span><br><span class="line"></span><br><span class="line">y = tf.matmul(x,w)</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"matmul:0"</span>,shape(<span class="number">1</span>,<span class="number">1</span>),dtype=float32)</span><br><span class="line">[[<span class="number">11.</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li>参数：即权重<code>w</code>，用变量表示，随机给初值。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(tf.random_normal([2,3], stddev=2, mean=0, seed=1))</span><br></pre></td></tr></table></figure>
<p>其中<code>tf.random_normal()</code>表示<code>正态分布</code>，<code>[2,3]</code>表示产生<code>2X3的矩阵</code>，<code>stddev</code>表示<code>标准差</code>，<code>mean</code>表示<code>均值</code>，<code>seed</code>表示<code>随机种子</code>;</p>
<p><code>tf.truncated_normal()</code>表示去掉过大偏离点的正态分布，<code>tf.random_uniform()</code>表示平均分布。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#tf.zeros全0数组</span><br><span class="line">tf.zeros([3,2],int32)  #生成[[0,0],[0,0],[0,0]]</span><br><span class="line"></span><br><span class="line">#tf.ones全1数组</span><br><span class="line">tf.ones([3,2],int32)  #生成[[1,1],[1,1],[1,1]]</span><br><span class="line"></span><br><span class="line">#tf.fill全定值数组</span><br><span class="line">tf.fll([3,2],6)    #生成[[6,6],[6,6],[6,6]]</span><br><span class="line"></span><br><span class="line">#tf.constant直接给值</span><br><span class="line">tf.constant([3,2,1]) # 生成[3,2,1]</span><br></pre></td></tr></table></figure>
<ul>
<li>神经网络实现过程：</li>
</ul>
<ol>
<li><p>准备数据集，提取特征，作为输入喂给神经网络<code>(Neural Network, NN)</code></p>
</li>
<li><p>搭建<code>NN</code>结构，从输入到输出(先搭建计算图，再用会话执行)</p>
<blockquote>
<p>NN前向传播算法 =&gt; 计算输出</p>
</blockquote>
</li>
<li><p>大量特征数据喂给<code>NN</code>，迭代优化<code>NN</code>参数</p>
<blockquote>
<p>NN反向传播算法 =&gt; 优化参数训练模型</p>
</blockquote>
</li>
<li><p>使用训练好的模型预测和分类</p>
</li>
</ol>
<ul>
<li>前向传播 =&gt; 搭建模型，实现推理（以全连接网络为例子）</li>
</ul>
<p>eg. 生产一批零件将体积x1和重量x2为特征输入NN，通过NN后输出一个数值。</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-17-15318051293124.jpg" alt=""></p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-17-15318054268584.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.matmul(X, W1)</span><br><span class="line">y = tf.matmul(a, W2)</span><br></pre></td></tr></table></figure>
<ul>
<li>变量初始化、计算图节点，运算都要用会话（with结构）实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run()</span><br></pre></td></tr></table></figure>
<ul>
<li>变量初始化：在<code>sess.run</code>函数中用<code>tf.global_variables_initializer()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line">sess.run(init_op)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算图节点运算：在<code>sess.run</code>函数中写入待运算的节点</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(y)</span><br></pre></td></tr></table></figure>
<ul>
<li>用<code>tf.placeholder</code>占位，在<code>sess.run</code>函数中用<code>feed_dict</code>喂数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 喂一组数据</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">sess.run(y, feed_dict=&#123;x:[[<span class="number">0.5</span>, <span class="number">0.6</span>]]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 喂多组数据</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">sess.run(y, feed_dict=&#123;x:[[<span class="number">0.1</span>,<span class="number">0.2</span>],[<span class="number">0.2</span>,<span class="number">0.3</span>],[<span class="number">0.3</span>,<span class="number">0.4</span>],[<span class="number">0.4</span>,<span class="number">0.5</span>]]&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># coding:utf-8</span><br><span class="line"># 两层简单神经网络(全连接)</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># 定义输入和参数</span><br><span class="line">x = tf.constant([[0.7,0.5]])</span><br><span class="line">W1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))</span><br><span class="line">W2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))</span><br><span class="line"></span><br><span class="line"># 定义前向传播过程</span><br><span class="line">a = tf.matmul(x, W1)</span><br><span class="line">y = tf.matmul(a, W2)</span><br><span class="line"></span><br><span class="line"># 用会话计算结果</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(&quot;y : \n&quot;, sess.run(y))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"># y : </span><br><span class="line"># [[ 3.0904665]]</span><br></pre></td></tr></table></figure>
<ul>
<li>参数：即权重w，用变量表示，随机给初值。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(tf.random_normal([2,3], stddev=2, mean=0, seed=1))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>其中tf.random_normal()表示正态分布，<br>[2,3]表示产生2X3的矩阵，<br>stddev表示标准差，<br>mean表示均值，<br>seed表示随机种子;<br>tf.truncated_normal()表示去掉过大偏离点的正态分布<br>tf.random_uniform()表示平均分布。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#tf.zeros全0数组</span><br><span class="line">tf.zeros([3,2],int32)  #生成[[0,0],[0,0],[0,0]]</span><br><span class="line"></span><br><span class="line">#tf.ones全1数组</span><br><span class="line">tf.ones([3,2],int32)  #生成[[1,1],[1,1],[1,1]]</span><br><span class="line"></span><br><span class="line">#tf.fill全定值数组</span><br><span class="line">tf.fll([3,2],6)    #生成[[6,6],[6,6],[6,6]]</span><br><span class="line"></span><br><span class="line">#tf.constant直接给值</span><br><span class="line">tf.constant([3,2,1]) # 生成[3,2,1]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 两层简单神经网络(全连接)</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line"><span class="comment"># 用placeholder定义输入(sess.run喂多维数据)</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">W1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">W2 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, W1)</span><br><span class="line">y = tf.matmul(a, W2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(<span class="string">"y: \n"</span>, sess.run(y, feed_dict=&#123;x:[[<span class="number">0.7</span>,<span class="number">0.5</span>],[<span class="number">0.2</span>,<span class="number">0.3</span>],[<span class="number">0.3</span>,<span class="number">0.4</span>],[<span class="number">0.4</span>,<span class="number">0.5</span>]]&#125;))</span><br><span class="line">    print(<span class="string">"W1: \n"</span>, sess.run(W1))</span><br><span class="line">    print(<span class="string">"W2: \n"</span>, sess.run(W2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">y: </span><br><span class="line"> [[ <span class="number">3.0904665</span> ]</span><br><span class="line"> [ <span class="number">1.2236414</span> ]</span><br><span class="line"> [ <span class="number">1.72707319</span>]</span><br><span class="line"> [ <span class="number">2.23050475</span>]]</span><br><span class="line">W1: </span><br><span class="line"> [[<span class="number">-0.81131822</span>  <span class="number">1.48459876</span>  <span class="number">0.06532937</span>]</span><br><span class="line"> [<span class="number">-2.4427042</span>   <span class="number">0.0992484</span>   <span class="number">0.59122431</span>]]</span><br><span class="line">W2: </span><br><span class="line"> [[<span class="number">-0.81131822</span>]</span><br><span class="line"> [ <span class="number">1.48459876</span>]</span><br><span class="line"> [ <span class="number">0.06532937</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li><p>反向传播 =&gt; 训练模型参数，在所有参数上用梯度下降，使<code>NN</code>模型在训练数据上的损失函数最小。</p>
</li>
<li><p>损失函数<code>loss</code>：预测值<code>y</code>与已知答案<code>y_</code>的差距</p>
</li>
<li><p>均方误差<code>MSE</code></p>
</li>
</ul>
<script type="math/tex; mode=display">MSE(y\_,y) = \frac{\sum_{i=1}^n(y-y\_)^2}{n}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.square(y_-y))</span><br></pre></td></tr></table></figure>
<ul>
<li>反向传播训练方法：以减小<code>loss</code>值为优化目标</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span><br><span class="line"></span><br><span class="line">train_step = tf.train.MomentumOptimizer(learning_rate).minimize(loss)</span><br><span class="line"></span><br><span class="line">train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)</span><br></pre></td></tr></table></figure>
<ul>
<li>学习率<code>learning_rate</code>：决定参数每次更新的幅度</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">#encoding:utf-8</span><br><span class="line">#0 导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">seed = 23455</span><br><span class="line"></span><br><span class="line"># 基于seed产生随机数</span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"></span><br><span class="line"># 随机数返回32行2列的矩阵 表示32组 体积和重量 作为输入数据集</span><br><span class="line">X = rng.rand(32,2)</span><br><span class="line"></span><br><span class="line"># 从X这个32行2列的矩阵中 取出一行 判断如果和小于1 给Y赋值1 如果不小于1 给Y赋值0</span><br><span class="line"># 作为输入数据集的标签(正确答案)</span><br><span class="line">Y = [[int(x0 + x1 &lt; 1)] for (x0,x1) in X]</span><br><span class="line">print(&quot;X:\n&quot;,X)</span><br><span class="line">print(&quot;Y:\n&quot;,Y)</span><br><span class="line"></span><br><span class="line">#1 定义神经网络的输入、参数和输出，定义前向传播过程</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))</span><br><span class="line"></span><br><span class="line">a = tf.matmul(x,w1)</span><br><span class="line">y = tf.matmul(a,w2)</span><br><span class="line"></span><br><span class="line">#2 定义损失函数及方向传播方法</span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)</span><br><span class="line">#train_step = tf.train.MomentumOptimizer(0.01,0.9).minimize(loss)</span><br><span class="line">#train_step = tf.train.AdamOptimizer(0.01).minimize(loss)</span><br><span class="line"></span><br><span class="line">#3 生成会话，训练STEP轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  # 输出目前(未经训练)的参数取值</span><br><span class="line">  print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">  print(&quot;w2:\n&quot;, sess.run(w2))</span><br><span class="line">  </span><br><span class="line">  # 训练模型</span><br><span class="line">  STEPS = 3000</span><br><span class="line">  for i in range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % 32</span><br><span class="line">    end = start + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x:X[start:end], y_: Y[start:end]&#125;)</span><br><span class="line">    if i % 500 == 0:</span><br><span class="line">      total_loss = sess.run(loss, feed_dict=&#123;x:X,y_:Y&#125;)</span><br><span class="line">      print(&quot;After %d training step(s), loss on all data is %g&quot; % (i, total_loss))</span><br><span class="line">      </span><br><span class="line">    # 输出训练后的参数取值</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;w2:\n&quot;, sess.run(w2))</span><br></pre></td></tr></table></figure>
<ul>
<li>神经元模型：用数学公式表示为: $f(\sum_ix_iw_i+b)$，$f$为激活函数。神经网络是以神经元为基本单元构成的。</li>
</ul>
<ul>
<li>激活函数：引入非线性激活因素，提高模型的表达力。</li>
</ul>
<blockquote>
<p>(1) 常用的激活函数<code>relu</code>：在Tensorflow中，用<code>tf.nn.relu()</code>表示</p>
</blockquote>
<script type="math/tex; mode=display">f(x)=max(x,0)=\begin{cases}0 & x <= 0 \\ x & x >= 0\end{cases}</script><p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-23-15323339395898.jpg" width="200"></p>
<blockquote>
<p>(2) 激活函数<code>sigmoid</code>：在Tensorflow中，用<code>tf.nn.sigmoid()</code>表示</p>
</blockquote>
<script type="math/tex; mode=display">f(x)=\frac{1}{1+e^{-x}}</script><p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-23-15323352101703.jpg" width="200"></p>
<blockquote>
<p>(3) 激活函数<code>tanh</code>：在tensorflow中，用<code>tf.nn.tanh()</code>表示</p>
</blockquote>
<script type="math/tex; mode=display">f(x)=\frac{1-e^{-2x}}{1+e^{-2x}}</script><p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-24-15324075893250.jpg" width="200"></p>
<ul>
<li><p>神经网络的复杂度：可用神经网络的层数和神经网络中待优化的参数个数表示</p>
</li>
<li><p>神经网络的层数：一般不计入输入层，层数=n个隐藏层+1个输出层</p>
</li>
<li><p>神经网络待优化的参数：神经网络中所有参数<code>w</code>的个数+所有参数<code>b</code>的个数</p>
</li>
</ul>
<blockquote>
<p>例如:</p>
</blockquote>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-24-15324078150692.jpg" width="200"></p>
<blockquote>
<p>该神经网络中，包含1个输入层、1个隐藏层和1个输出层，该神经网络的层数为2层。<br>在该神经网络中，参数的个数是所有参数<code>w</code>的个数加上所有参数<code>b</code>的总数，第一层参数用3行4列的二阶张量表示（即12个线上的权重<code>w</code>）再加4个偏置<code>b</code>；第二层参数是4行2列的二阶张量（即8个线上的权重<code>w</code>）在加上2个偏置<code>b</code>。总参数=3X4+4+4X2+2=26</p>
</blockquote>
<ul>
<li><p>损失函数<code>loss</code>：用来表示预测值<code>y</code>与已知答案<code>y_</code>的差距。在训练神经网络时，通过不断改变神经网络中所有参数，使损失函数不断减小，从而训练出更高准确率的神经网络模型。</p>
</li>
<li><p>常用的损失函数有均方误差、自定义和交叉熵等。</p>
</li>
<li><p>均方误差<code>mse</code>：<code>n</code>个样本的预测值<code>y</code>与已知答案<code>y_</code>的平方和，再求均值。</p>
</li>
</ul>
<script type="math/tex; mode=display">MSE(y\_,y) = \frac{\sum_{i=1}^n(y - y\_)^2}{n}</script><blockquote>
<p>在Tensorflow中用</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_mse = tf.reduce_mean(tf.square(y_-y))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>例如：预测酸奶日销量<code>y</code>,<code>x1</code>,<code>x2</code>是影响销量的两个因素。<br>应提前采集的数据有：一段时间内，每日的<code>x1</code>因素、<code>x2</code>因素和销量<code>y_</code>。采集的数据尽量多。本例中用销量预测产量，最优的产量应该等于销量。由于目前没有数据集，所以拟造了一套数据集。利用Tensorflow中函数随机生成<code>x1</code>,<code>x2</code>，制造标准答案<code>y_=x1+x2</code>，为了更真实，求和后还加了正负0.05的随机噪声。我们把这套自制的数据集喂入神经网络，构建一个一层的神经网络，拟合预测酸奶日销量的函数。</p>
</blockquote>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#预测多或少的影响一样</span></span><br><span class="line"><span class="comment">#0 导入模块，生成数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">Y = [[x1+x2+(rdm.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment">#1 定义神经网络的输入、参数和输出，定义前向传播的过程。</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2 定义损失函数及反向传播方法。</span></span><br><span class="line"><span class="comment"># 定义损失函数为MSE，反向传播方法为梯度下降。</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y_-y))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss_mes)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3 生成会话，训练STEPS轮</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  STEPS = <span class="number">20000</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">    end = (i*BATCH_SIZE) % <span class="number">32</span> + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y_[start:end]&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">      print(<span class="string">"After %d training steps, w1 is: "</span> % (i))</span><br><span class="line">      print(sess.run(w1),<span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"Final w1 is: \n"</span>, sess.run(w1))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">After 19000 training steps, w1 is:</span><br><span class="line">[[0.974931  ]</span><br><span class="line"> [1.02062762]]</span><br><span class="line"> </span><br><span class="line">After 19500 training steps, w1 is:</span><br><span class="line">[[0.97770262]</span><br><span class="line"> [1.01819491]]</span><br><span class="line"> </span><br><span class="line">Final w1 is：</span><br><span class="line">[[0.98019385]</span><br><span class="line"> [1.0159872 ]]</span><br></pre></td></tr></table></figure>
<p>由上述代码可知，本例中神经网络预测模型为 <script type="math/tex">y=w_1x_1+w_2x_2</script> ，损失函数采用均方误差。通过使损失函数值<code>loss</code>不断降低，神经网络模型得到最终参数<script type="math/tex">x_{1}=0.98, w_{2}=1.02</script>，销量预测结果为<script type="math/tex">y=0.98*x_1+1.02*x_2</script>。由于在生成数据集时，标准答案为<script type="math/tex">y=x_1+x_2</script>，因此，销量预测结果和标准答案已非常接近，说明该神经网络预测正确。</p>
<ul>
<li>自定义损失函数：根据问题的实际情况，定制合理的损失函数。</li>
</ul>
<p>例如：对于预测酸奶日销量问题，如果预测销量大于实际销量会损失成本；如果预测销量小于实际销量则会损失利润。在实际生活中，往往制造一盒酸奶的成本和销售一盒酸奶的利润是不等价的。因此，需要使用符合该问题的自定义损失函数。</p>
<p>自定义损失函数为：<script type="math/tex">loss=\sum_{n}f(y\_,y)</script><br>其中，损失定义成分段函数：</p>
<script type="math/tex; mode=display">f(y\_，y)=\begin{cases}PROFIT*(y\_-y) & y < y\_\\ COST*(y-y\_) & y \geq y\_\end{cases}</script><p>损失函数表示，若预测结果<code>y</code>小于标准答案<code>y_</code>，损失函数为利润乘以预测结果<code>y</code>与标准答案<code>y_</code>之差；若预测结果<code>y</code>大于标准答案<code>y_</code>，损失函数为成本乘以预测结果<code>y</code>与标准答案<code>y_</code>之差。</p>
<p>用Tensorflow函数表示为:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y,y_), COST(y,y_), PROFIT(y_,y)))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(1)若酸奶成本为1元，酸奶销售利润为9元，则制造成本小于酸奶利润，因此希望预测的结果<code>y</code>多一些。采用上述的自定义损失函数，训练神经网络。</p>
</blockquote>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#酸奶成本1元，酸奶利润9元</span><br><span class="line">#预测少了损失，故不要预测少，故生成的模型会多预测一些</span><br><span class="line">#0 导入模块，生成数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">SEED = 23455</span><br><span class="line">COST = 1</span><br><span class="line">PROFIT = 9</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(32,2)</span><br><span class="line">Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in X]</span><br><span class="line"></span><br><span class="line">#1 定义神经网络的输入、参数和输出，定义前向传播的过程。</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))</span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line">#2 定义损失函数及反向传播方法。</span><br><span class="line"># 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。</span><br><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y,y_)*COST,(y_-y)*PROFIT))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)</span><br><span class="line"></span><br><span class="line">#3 生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  STEPS = 20000</span><br><span class="line">  for i in range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % 32</span><br><span class="line">    end = (i*BATCH_SIZE) % 32 + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y_[start:end]&#125;)</span><br><span class="line">    if i % 500 == 0:</span><br><span class="line">      print(&quot;After %d training steps, w1 is: &quot; % (i))</span><br><span class="line">      print(sess.run(w1),&quot;\n&quot;)</span><br><span class="line">print(&quot;Final w1 is: \n&quot;, sess.run(w1))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">After 2000 training steps, w1 is:</span><br><span class="line">[[1.01793861]</span><br><span class="line"> [1.04128993]]</span><br><span class="line"> </span><br><span class="line">After 2500 training steps, w1 is:</span><br><span class="line">[[1.02059376]</span><br><span class="line"> [1.03906775]]</span><br><span class="line"> </span><br><span class="line">Final w1 is:</span><br><span class="line">[[1.02965927]</span><br><span class="line"> [1.0484432 ]]</span><br></pre></td></tr></table></figure>
<p>由代码执行结果可知，神经网络最终参数为 $w_1=1.03$，$ w_2=1.05$，销量预测结果为 $y =1.03x_1 + 1.05x_2$。由此可见，采用自定义损失函数预测的结果大于采用均方误差预测的结果，更符合实际需求。</p>
<blockquote>
<p>(2)若酸奶成本为9元，酸奶销售利润为1元，则制造成本大于酸奶利润，因此希望预测结果$y$小一 些。采用上述的自定义损失函数，训练神经网络模型。</p>
</blockquote>
<p>代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#酸奶成本1元，酸奶利润9元</span><br><span class="line">#预测少了损失，故不要预测少，故生成的模型会多预测一些</span><br><span class="line">#0 导入模块，生成数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">SEED = 23455</span><br><span class="line">COST = 9</span><br><span class="line">PROFIT = 1</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(32,2)</span><br><span class="line">Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in X]</span><br><span class="line"></span><br><span class="line">#1 定义神经网络的输入、参数和输出，定义前向传播的过程。</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))</span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line">#2 定义损失函数及反向传播方法。</span><br><span class="line"># 定义损失函数使得预测多了的损失大，于是模型应该偏向少的方向预测。</span><br><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y,y_)*COST,(y_-y)*PROFIT))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)</span><br><span class="line"></span><br><span class="line">#3 生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  STEPS = 20000</span><br><span class="line">  for i in range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % 32</span><br><span class="line">    end = (i*BATCH_SIZE) % 32 + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y_[start:end]&#125;)</span><br><span class="line">    if i % 500 == 0:</span><br><span class="line">      print(&quot;After %d training steps, w1 is: &quot; % (i))</span><br><span class="line">      print(sess.run(w1),&quot;\n&quot;)</span><br><span class="line">print(&quot;Final w1 is: \n&quot;, sess.run(w1))</span><br></pre></td></tr></table></figure>
<p>运行结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">After 2000 training steps, w1 is:</span><br><span class="line">[[0.96024752]</span><br><span class="line"> [0.97420841]]</span><br><span class="line"></span><br><span class="line">After 2500 training steps, w1 is:</span><br><span class="line">[[0.96100295]</span><br><span class="line"> [0.96993417]]</span><br><span class="line"></span><br><span class="line">Final w1 is</span><br><span class="line">[[0.96004069]</span><br><span class="line"> [0.97334176]]</span><br></pre></td></tr></table></figure>
<p>由执行结果可知，神经网络最终参数为$w1=0.96$，$w2=0.97$，销量预测结果为$y=0.96x_1 + 0.97x_2$。 因此，采用自定义损失函数预测的结果小于采用均方误差预测的结果，更符合实际需求。</p>
<ul>
<li>交叉熵<code>Cross Entropy</code>:表示两个概率分布之间的距离。交叉熵越大，两个概率分布距离越远，两个概率分布越相异;交叉熵越小，两个概率分布距离越近，两个概率分布越相似。 交叉熵计算公式:<script type="math/tex">H(y\_,y)=-\sum{y\_\log{y}}</script></li>
</ul>
<p>用 Tensorflow 函数表示为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ce = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-2,1.0)))</span><br></pre></td></tr></table></figure>
<p>例如:两个神经网络模型解决二分类问题中，已知标准答案为<code>y_=(1,0)</code>，第一个神经网络模型预测结果为<code>y1=(0.6,0.4)</code>，第二个神经网络模型预测结果为<code>y2=(0.8,0.2)</code>，判断哪个神经网络模型预测的结果更接近标准答案？</p>
<p>根据交叉熵的计算公式得:</p>
<p><code>H1((1,0),(0.6,0.4)) = -(1*log0.6 + 0*log0.4) ≈ -(-0.222 + 0) = 0.222</code></p>
<p><code>H2((1,0),(0.8,0.2)) = -(1*log0.8 + 0*log0.2) ≈ -(-0.097 + 0) = 0.097</code></p>
<p>由于<code>0.222 &gt; 0.097</code>，所以预测结果<code>y2</code>与标准答案<code>y_</code>更接近，<code>y2</code>预测更准确。</p>
<ul>
<li>Softmax函数：将<code>n</code>分类的<code>n</code>个输出<code>(y1,y2,...,yn)</code>变为满足以下概率分布要求的函数。</li>
</ul>
<p>$\forall{x}$ 有 $P(X=x)\in[0,1]$ 且 $\sum{P_x(X=x)}=1$</p>
<p>softmax函数表示为<script type="math/tex">softmax(y_i)=\frac{e^{y_{i}}}{\sum_{j=1}^{n}e^{y_{i}}}</script></p>
<p>softmax函数应用：在<code>n</code>分类中，模型会有<code>n</code>个输出，即y1,y2,…,yn，其中yi表示第i中情况的可能性大小。将<code>n</code>个输出经过softmax函数，可得到符合概率分布的分类结果。</p>
<ul>
<li>在Tensorflow中，一般让模型经过softmax函数，以获得输出分类的概率分布，再与标准答案对比，求出交叉熵，得到损失函数，用如下函数实现：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))</span><br><span class="line">cem = tf.reduce_mean(ce)</span><br></pre></td></tr></table></figure>
<ul>
<li>学习率<code>learning_rate</code>:表示了每次参数更新的幅度大小。学习率过大，会导致待优化的参数在最小值附近波动，不收敛；学习率过小，会导致待优化的参数收敛缓慢。在训练过程中，参数的更新向着损失函数梯度下降的方向。参数的更新公式为<script type="math/tex; mode=display">w_{n+1}=w_{n}-learning\_rate\Delta</script></li>
</ul>
<p>假设损失函数为$loss=(w+1)^2$。梯度是损失函数<code>loss</code>的导数$\Delta=2w+2$。如参数初值为5，学习率为0.2，则参数和损失函数更新如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1次  参数w:5       5-0.2*(2*5+2)=2.6</span><br><span class="line">2次  参数w:2.6     2.6-0.2*(2*2.6+2)=1.16</span><br><span class="line">3次  参数w:1.16    1.16-0.2*(2*1.16+2)=0.296</span><br><span class="line">4次  参数w:0.296</span><br></pre></td></tr></table></figure>
<p>损失函数$loss=(w+1)^2$的图像为：</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-26-15325368607915.jpg" width="200"></p>
<p>由图可知，损失函数loss的最小值会在(-1,0)处得到，此时损失函数的导数为0,得到最终参数w=-1。代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#encoding:utf-8</span><br><span class="line">#设损失函数loss=(w+1)^2，令w初值是常数5。反向传播就是求最优y，即求最小loss对应的w值</span><br><span class="line">import tensorflow as tf</span><br><span class="line">#定义待优化参数w初值赋5</span><br><span class="line">w = tf.Variable(tf.constant(5,dtype=tf.float32))</span><br><span class="line"></span><br><span class="line">#定义损失函数loss</span><br><span class="line">loss = tf.square(w+1)</span><br><span class="line">#定义反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#生成会话，训练40轮</span><br><span class="line">with tf.Session() as tf:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  for i in rang(40):</span><br><span class="line">    sess.run(train_step)</span><br><span class="line">    w_val = sess.run(w)</span><br><span class="line">    loss_val = sess.run(loss)</span><br><span class="line">    print(&quot;After %s steps: w is %f, loss is %f.&quot; % (i, w_val, loss_val))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">After 30 steps: w is -0.999999, loss is 0.000000 </span><br><span class="line">After 31 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 32 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 33 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 34 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 35 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 36 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 37 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 38 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 39 steps: w is -1.000000, loss is 0.000000</span><br></pre></td></tr></table></figure>
<p>由结果可知，随着损失函数值的减小，<code>w</code>无限趋近于-1，模型计算推测出最优参数<code>w=-1</code>。</p>
<ul>
<li>学习率的设置，学习率过大，会导致待优化的参数在最小值附近波动，不收敛；学习率过小，会导致优化参数收敛缓慢</li>
</ul>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-26-sgd.gif" alt="sgd"><br><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-26-sgd_bad.gif" alt="sgd_bad"></p>
<p>例如：(1)对于上例的损失函数$loss=(w+1)^2$。则将上述代码中学习率修改为1，其余内容不变。实验结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">After 11 steps: w is 5.000000, loss is 36.000000 </span><br><span class="line">After 12 steps: w is -7.000000, loss is 36.000000</span><br><span class="line">After 13 steps: w is 5.000000, loss is 36.000000</span><br><span class="line">After 14 steps: w is -7.000000, loss is 36.000000</span><br><span class="line">After 15 steps: w is 5.000000, loss is 36.000000</span><br><span class="line">After 16 steps: w is -7.000000, loss is 36.000000</span><br></pre></td></tr></table></figure>
<p>运行结果可知，损失函数<code>loss</code>值并没有收敛，而是在5和-7之间波动。</p>
<p>（2）对于上例的损失函数$loss=(w+1)^2$。则将上述代码中学习率修改为0.0001，其余内容不变。实验结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">After 31 steps: w is 4.961716, loss is 35.542053 </span><br><span class="line">After 32 steps: w is 4.960523, loss is 35.527836</span><br><span class="line">After 33 steps: w is 4.959311, loss is 35.513626</span><br><span class="line">After 34 steps: w is 4.958139, loss is 35.499420</span><br><span class="line">After 35 steps: w is 4.956947, loss is 35.485222</span><br><span class="line">After 36 steps: w is 4.955756, loss is 35.471027</span><br><span class="line">After 37 steps: w is 4.954565, loss is 35.456841</span><br><span class="line">After 38 steps: w is 4.953373, loss is 35.442654</span><br><span class="line">After 39 steps: w is 4.952183, loss is 35.428478</span><br></pre></td></tr></table></figure>
<p>由运行结果可知，损失函数<code>loss</code>值缓慢下降，<code>w</code>值也在小幅度变化，收敛缓慢。</p>
<ul>
<li>指数衰减学习率: 学习率随着训练轮数变化而动态更新</li>
</ul>
<script type="math/tex; mode=display">Learning\_rate=LEARNING\_RATE\_BASE*LEARNING\_RATE\_DECY*\frac{global\_step}{LEARNING\_RATE\_BATCH\_SIZE}</script><p>用Tensorflow的函数表示为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Varibale(0, trainable=False)</span><br><span class="line">learning_rate = tf.train.exponential_decy(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP,LEARNING_RATE_DECAY,staircase=True/False)</span><br></pre></td></tr></table></figure>
<p>其中，<code>LEARNING_RATE_BASE</code>为学习率初始值，<code>LEARNING_RATE_DECAY</code>为学习率衰减率,<code>global_step</code>记录了当前训练轮数，为不可训练型参数。学习率<code>learning_rate</code>更新频率为输入数据集总样本数除以每次喂入样本数。若<code>staircase</code>设置为<code>True</code>时，表示 <code>global_step/learning_rate_step</code>取整数，学习率阶梯型衰减;若<code>staircase</code>设置为<code>false</code>时，学习率会是一条平滑下降的曲线。</p>
<p>例如:在本例中，模型训练过程不设定固定的学习率，使用指数衰减学习率进行训练。其中，学习率初值设置为 0.1，学习率衰减率设置为0.99，BATCH_SIZE设置为1。</p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#encoding:utf-8</span><br><span class="line">#设损失函数loss=(w+2)^2，令w初值是常数10。反向传播就是求最优w，即求最小loss对应的w值</span><br><span class="line">#使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得更有收敛度。</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = 0.1   #初始学习率</span><br><span class="line">LEARNING_RATE_DECAY = 0.99 #学习衰减率</span><br><span class="line">LEARNING_RATE_STEP = 1     #喂入多少轮BATCH_SIZE后，更新一次学习率，一般设为：总样本数/BATCH_SIZE</span><br><span class="line"></span><br><span class="line">#运行了几轮BATCH_SIZE的计数器，初值给0，设为不被训练</span><br><span class="line">global_step = tf.Variable(0, trainable=False)</span><br><span class="line">#定义指数下降学习率</span><br><span class="line">learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP, LEARNING_RATE_DECAY, staircase=True)</span><br><span class="line">#定义待优化参数，初值给10</span><br><span class="line">w = tf.Variable(tf.constant(5, dtype=tf.float32))</span><br><span class="line">#定义损失函数loss</span><br><span class="line">loss = tf.square(w+1)</span><br><span class="line">#定义反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line">#生成会话训练40轮</span><br><span class="line">with tf.Session() as tf:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  for i in range(40):</span><br><span class="line">    sess.run(train_step)</span><br><span class="line">    learning_rate_val = sess.run(learning_rate)</span><br><span class="line">    global_step_val = sess.run(global_step)</span><br><span class="line">    w_val = sess.run(w)</span><br><span class="line">    loss_val = sess.run(loss)</span><br><span class="line">    print(&quot;After %s steps: global_step is %f, w is %f, learning rate is %f, loss is %f&quot; % (i, global_step_val, w_val, learning_rate_val, loss_val))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">After 35 steps: global_step is 36.000000, w is -0.992297, learning_rate is 0.069641, loss is 0.000059</span><br><span class="line">After 36 steps: global_step is 37.000000, w is -0.993369, learning_rate is 0.068945, loss is 0.000044</span><br><span class="line">After 37 steps: global_step is 38.000000, w is -0.994284, learning_rate is 0.068255, loss is 0.000033</span><br><span class="line">After 38 steps: global_step is 39.000000, w is -0.995064, learning_rate is 0.067573, loss is 0.000024</span><br><span class="line">After 39 steps: global_step is 40.000000, w is -0.995731, learning_rate is 0.066897, loss is 0.000018</span><br></pre></td></tr></table></figure>
<p>由结果可以看出，随着训练轮数增加学习率在不断减小。</p>
<ul>
<li>滑动平均:记录了一段时间内模型中所有参数<code>w</code>和<code>b</code>各自的平均值。利用滑动平均值可以增强模型的泛化能力。</li>
<li>滑动平均值(影子)计算公式:<code>影子=衰减率*影子+(1-衰减率)*参数</code>;其中，</li>
</ul>
<script type="math/tex; mode=display">衰减率=min\{MOVING_{AVERAGE_{DECAY'}}\frac{1+轮数}{10+轮数}\}</script><p>影子初值=参数初值</p>
<ul>
<li>用Tensorflow函数表示为：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, gloabl_step)</span><br></pre></td></tr></table></figure>
<p>其中，<code>MOVING_AVERAGE_DECAY</code>表示滑动平均衰减率，一般会赋接近1的值，<code>global_step</code>表示当前训练了多少轮。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br></pre></td></tr></table></figure>
<p>其中，<code>ema.apply()</code>函数实现对括号内参数求滑动平均，<code>tf.trainable_variables()</code>函数实现把所有 待训练参数汇总为列表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.control_dependencies([train_step, ema_op]):  train_op = tf.no_op(name=&apos;train&apos;)</span><br></pre></td></tr></table></figure>
<p>其中，该函数实现将滑动平均和训练过程同步运行。</p>
<p>查看模型中参数的平均值，可以用 <code>ema.average()</code>函数。</p>
<p>例如:<br>在神经网络模型中，将<code>MOVING_AVERAGE_DECAY</code>设置为0.99，参数<code>w1</code>设置为0，<code>w1</code>的滑动平均值设 置为0。</p>
<p>(1)开始时，轮数<code>global_step</code>设置为0，参数<code>w1</code>更新为1，则<code>w1</code>的滑动平均值为:<br><code>w1</code>，<code>滑动平均值=min(0.99,1/10)*0+(1– min(0.99,1/10)*1 = 0.9</code></p>
<p>(2)当轮数<code>global_step</code>设置为100时，参数<code>w1</code>更新为10，以下代码<code>global_step</code>保持为100，每次执行滑动平均操作影子值更新，则滑动平均值变为:<code>w1滑动平均值=min(0.99,101/110)*0.9+(1– min(0.99,101/110)*10 = 0.826+0.818=1.644</code></p>
<p>(3)再次运行，参数<code>w1</code>更新为1.644，则滑动平均值变为:<code>w1滑动平均值=min(0.99,101/110)*1.644+(1– min(0.99,101/110)*10 = 2.328</code></p>
<p>(4)再次运行，参数<code>w1</code>更新为2.328，则滑动平均值:<code>w1滑动平均值=2.956</code></p>
<p>代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">#encoding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#1. 定义变量及滑动平均类</span><br><span class="line"># 定义一个32位浮点变量，初始值为0.0 这个代码就是不断更新w1参数，优化w1参数，滑动平均做了个w1的影子</span><br><span class="line">w1 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">#定义num_updates(NN的迭代轮数)，初始值为0，不可被优化（训练），这个参数不训练</span><br><span class="line">global_step = tf.Variable(0, trainable=False)</span><br><span class="line"></span><br><span class="line">#实例化滑动平均类，给删减率为0.99，当前轮global_step</span><br><span class="line">MOVING_AVERAGE_DECAY = 0.99</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line"></span><br><span class="line">#ema.apply后的括号里是更新列表，每次运行sess.run(ema_op)时，对更新列表中的元素求滑动平均值</span><br><span class="line">#在实际应用中会使用tf.trainable_variables()自动将所有待训练的参数汇总为列表</span><br><span class="line">#ema_op = ema.apply([w1])</span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line">#2. 查看不同迭代中变量取值的变化</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  #初始化</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  #用ema.average(w1)获取w1滑动平均值（要运行多个节点，作为列表中的元素列出，写在sess.run中）</span><br><span class="line">  #打印出当前参数w1和w1滑动平均值</span><br><span class="line">  print(sess.run([w1, ema.average(w1)]))</span><br><span class="line">  </span><br><span class="line">  #参数w1的值赋为1</span><br><span class="line">  sess.run(tf.assign(w1,1))</span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line">  </span><br><span class="line">  #更新step的w1的值，模拟出100轮迭代后，参数w1变为10</span><br><span class="line">  sess.run(tf.assign(global_step, 100))</span><br><span class="line">  sess.run(tf.assign(w1, 10))</span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1, ema.average(w1)]))</span><br><span class="line">  </span><br><span class="line">  #每次sess.run会更新一次w1的滑动平均值</span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line">  </span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[0.0, 0.0]</span><br><span class="line">[1.0, 0.89999998]</span><br><span class="line">[10.0, 1.6445453]</span><br><span class="line">[10.0, 2.3281732]</span><br><span class="line">[10.0, 2.955868]</span><br><span class="line">[10.0, 3.5322061]</span><br><span class="line">[10.0, 4.061389]</span><br><span class="line">[10.0, 4.5472751]</span><br><span class="line">[10.0, 4.9934072]</span><br></pre></td></tr></table></figure>
<p>从运行结果可知，最初参数<code>w1</code>和滑动平均值都是0;参数<code>w1</code>设定为1后，滑动平均值变为0.9;当迭代轮数更新为100轮时，参数<code>w1</code>更新为10后，滑动平均值变为1.644。随后每执行一次，参数<code>w1</code>的滑动平均值都向参数<code>w1</code>靠近。可见，滑动平均追随参数的变化而变化。</p>
<ul>
<li>过拟合：神经网络模型在训练数据集上的准确率较高，在新的数据进行预测或分类时准确率较低，说明模型的泛化能力差。</li>
<li>正则化：在损失函数中给每个参数<code>w</code>加上权重，引入模型复杂度指标，从而抑制模型噪声，减小过拟合。</li>
</ul>
<p>使用正则化后，损失函数<code>loss</code>变为两项之和：</p>
<p><code>loss=loss(y与y_)+REGULARIZER*loss(w)</code></p>
<p>其中，第一项是预测结果与标准答案之间的差距，如之前讲过的交叉熵、均方误差等；第二项是正则化计算结果。</p>
<ul>
<li>正则化计算方法：</li>
</ul>
<p>(1) <code>L1</code>正则化: <script type="math/tex">loss_{L1}=\sum_{i}|w_{i}|</script></p>
<p>用Tensorflow函数表示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w)</span><br></pre></td></tr></table></figure>
<p>(2) <code>L2</code>正则化: <script type="math/tex">loss_{L2}=\sum_{i}|w_{i}|^2</script><br>用Tensorflow函数表示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w)</span><br></pre></td></tr></table></figure>
<ul>
<li>用Tensorflow函数实现正则化：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.add_to_collection(&apos;losses&apos;,tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">loss = cem + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br></pre></td></tr></table></figure>
<p>其中<code>cem</code>见前面提到的。</p>
<p>例如：用300个符合正态分布的点<script type="math/tex">X[x_{0},x_{1}]</script>作为数据集，根据点<script type="math/tex">X[x_{0},x_{1}]</script>计算生成标注<code>Y_</code>，将数据集标注为红色点和蓝色点。标注规则为：当<script type="math/tex">x_{0}^2 + x_{1}^2 < 2</script>时，<code>y_=1</code>，标注为红色；当<script type="math/tex">x_{0}^2 + x_{1}^2 \geq 2</script>时，<code>y_=0</code>标注为蓝色。我们分别用无正则化和有正则化两种方法拟合曲线，把红色点和蓝色点分开。在实际分类时，如果前向传播输出的预测值<code>y</code>接近1则为红色点概率越大，接近0则为蓝色点概率越大，输出的预测值<code>y</code>为0.5是红蓝点概率分界线。</p>
<p>在本例子中，我们使用了之前未用过的模块与函数:</p>
<ul>
<li><p><code>matplotlib</code>模块：Python中的可视化工具模块，实现函数可视化，终端安装指令: <code>sudo pip install matplotlib</code></p>
</li>
<li><p>函数<code>plt.scatter()</code>:利用指定颜色实现点<code>(x,y)</code>的可视化</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x坐标, y坐标, c=&quot;颜色&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>本例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#0导入模块，生成模拟数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">BATCH_SIZE = <span class="number">30</span></span><br><span class="line">seed = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#基于seed产生随机数</span></span><br><span class="line">rdm = np.random.RandomState(seed)</span><br><span class="line"><span class="comment">#随机数返回300行2列的矩阵，表示300组坐标点(x0,x1)作为输入数据集</span></span><br><span class="line">X = rdm.randn(<span class="number">300</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#从X这个300行2列的矩阵中取出一行，判断如果两个坐标的平方和小于2，给Y赋值1，其余赋值0</span></span><br><span class="line"><span class="comment">#作为输入数据集的标签（正确答案）</span></span><br><span class="line">Y = [int(x0*x0+x1*x1 &lt; <span class="number">2</span>) <span class="keyword">for</span> (x0,x1) <span class="keyword">in</span> X]</span><br><span class="line"><span class="comment">#遍历Y中的每个元素，1赋值'red'其余赋值'blue'，这样可视化显示时人可以直观区分</span></span><br><span class="line">Y_c = [[<span class="string">'red'</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">'blue'</span>] <span class="keyword">for</span> y <span class="keyword">in</span> Y_]</span><br><span class="line"><span class="comment">#对数据集X和标签Y进行shape整理，第一个元素为-1表示，随第二个参数计算得到，第二个元素表示</span></span><br><span class="line"><span class="comment">#多少列，把X整理为n行2列，把Y整理为n行1列</span></span><br><span class="line">X = np.vstack(X).reshape(<span class="number">-1</span>,<span class="number">2</span>)</span><br><span class="line">Y_ = np.vstack(Y_).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">print(X)</span><br><span class="line">print(Y_)</span><br><span class="line">print(Y_c)</span><br><span class="line"><span class="comment">#用plt.scatter画出数据集X各行中第0列元素和第1列元素的点即各行的(x0,y0)，用各行Y_c对应的</span></span><br><span class="line"><span class="comment">#值表示颜色(c是color的缩写)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=np.squeeze(Y_c))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义神经网络的输入、参数和输出，定义前向传播过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape,regularizer)</span>:</span></span><br><span class="line">  w = tf.Variable(tf.random_normal(shape),dtype=tf.float32)</span><br><span class="line">  tf.add_to_collection(<span class="string">'losses'</span>,tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">  <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">  b = tf.Variable(tf.constant(<span class="number">0.01</span>,shape=shape))</span><br><span class="line">  <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = get_weight([<span class="number">2</span>,<span class="number">11</span>],<span class="number">0.01</span>)</span><br><span class="line">b1 = get_bias([<span class="number">11</span>])</span><br><span class="line">y1 = tf.nn.relu(tf.matmul(x,w1)+b1)</span><br><span class="line"></span><br><span class="line">w2 = get_weight([<span class="number">11</span>,<span class="number">1</span>],<span class="number">0.01</span>)</span><br><span class="line">b2 = get_bias([<span class="number">1</span>])</span><br><span class="line">y = tf.matmul(y1,w2)+b2 <span class="comment">#输出层不过激活</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">loss_total = loss_mse + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义反向传播方法：不含正则化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  STEPS = <span class="number">40000</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % <span class="number">300</span></span><br><span class="line">    end = start + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">      loss_mse_v = sess.run(loss_mse, feed_dict=&#123;x:X, y_:Y_&#125;)</span><br><span class="line">      print(<span class="string">"After %d steps, loss is: %f"</span> % (i,loss_mse_v))</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#xx在-3到3之间以步长0.01，yy在-3到3之间以步长0.01，生成二维网络坐标点</span></span><br><span class="line">  xx, yy = np.mgrid(<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>)</span><br><span class="line">  <span class="comment">#将xx，yy拉直，并合并成一个2列的矩阵，得到一个网络坐标点的集合</span></span><br><span class="line">  grid = np.c_[XX.ravel(),yy.ravel()]</span><br><span class="line">  <span class="comment">#将网络坐标点喂入神经网络，probs为输出</span></span><br><span class="line">  probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">  <span class="comment">#probs的shape调整成xx的样子</span></span><br><span class="line">  probs = probs.reshape(xx.shape)</span><br><span class="line">  print(<span class="string">"w1:\n"</span>, sess.run(w1))</span><br><span class="line">  print(<span class="string">"b1:\n"</span>, sess.run(b1))</span><br><span class="line">  print(<span class="string">"w2:\n"</span>, sess.run(w2))</span><br><span class="line">plt.scatter(X[:<span class="number">0</span>],X[:,<span class="number">1</span>],c=np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx,yy,probs,levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义反向传播方法: 包含正则化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_total)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  STEPS = <span class="number">40000</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % <span class="number">300</span></span><br><span class="line">    end = start + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">      loss_v = sess.run(loss_total, feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line">      print(<span class="string">"After %d steps, loss is: %f"</span> %(i, loss_v))</span><br><span class="line">    </span><br><span class="line">    xx, yy = np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    print(<span class="string">"w1:\n"</span>, sess.run(w1))</span><br><span class="line">    print(<span class="string">"b1:\n"</span>, sess.run(b1))</span><br><span class="line">    print(<span class="string">"w2:\n"</span>, sess.run(w2))</span><br><span class="line">    print(<span class="string">"b2:\n"</span>, sess.run(b2))</span><br><span class="line">plt.scatter(X[:<span class="number">0</span>],X[:,<span class="number">1</span>],c=np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx,yy,probs,levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>执行代码，效果如下：</p>
<p>首先，数据集实现可视化，<script type="math/tex">x_{0}^2+x_{1}^2 < 2</script>的点显示红色，<script type="math/tex">x_{0}^2+x_{1}^2 \geq 2</script>的点显示蓝色，如图所示：</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328790917732.jpg" width="400"></p>
<p>接着，执行无正则化的训练过程，把红色的点和蓝色的点分开，生成曲线如下图所示：</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328792396453.jpg" width="400"></p>
<p>最后，执行有正则化的训练过程，把红色的点和蓝色的点分开，生成曲线如下图所示：</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328793451699.jpg" width="400"></p>
<p>对比无正则化与有正则化模型的训练结果，可看出有正则化模型的拟合曲线平滑，模型具有更好的泛化能力。</p>
<h2 id="搭建模块化神经网络"><a href="#搭建模块化神经网络" class="headerlink" title="搭建模块化神经网络"></a>搭建模块化神经网络</h2><ul>
<li>前向传播：由输入到输出，搭建完整的网络结构</li>
</ul>
<p>描述前向传播的过程需要定义三个函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def forward(x, regularizer):</span><br><span class="line">  w = </span><br><span class="line">  b = </span><br><span class="line">  y = </span><br><span class="line">  return y</span><br></pre></td></tr></table></figure>
<p>第一个函数<code>forward()</code>完成网络结构的设计，从输入到输出搭建完整的网络结构，实现前向传播过程。该函数中，参数<code>x</code>为输入，<code>regularizer</code>为正则化权重，返回值为预测或分类结果<code>y</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">  w = tf.Variable()</span><br><span class="line">  tf.add_to_collection(&apos;losses&apos;, tf.contrib.l2_regularizer(regularizer)(w))</span><br><span class="line">  return w</span><br></pre></td></tr></table></figure>
<p>第二个函数<code>get_weight()</code>对参数<code>w</code>设定。该函数中，参数<code>shape</code>表示参数<code>w</code>的形状，<code>regularizer</code>表示正则化权重，返回值为参数<code>w</code>。其中，<code>tf.Variable()</code>给<code>w</code>赋初值，<code>tf.add_to_collection()</code>表示将参数<code>w</code>正则化损失加到总损失<code>losses</code>中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def get_bias(shape):</span><br><span class="line">  b = tf.Variable()</span><br><span class="line">  return b</span><br></pre></td></tr></table></figure>
<p>第三个函数<code>get_bias()</code>对参数<code>b</code>进行设定。该函数中，参数<code>shape</code>表示参数<code>b</code>的形状，返回值为参数<code>b</code>。其中，<code>tf.Variable()</code>表示给<code>b</code>赋初值。</p>
<ul>
<li>反向传播：训练网络，优化网络参数，提高模型准确性。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def backward():</span><br><span class="line">  x = tf.placeholder()</span><br><span class="line">  y_ = tf.placeholder()</span><br><span class="line">  y = forward.forward(x, REGULARIZER)</span><br><span class="line">  global_step = tf.Varibale(0, trainable=False)</span><br><span class="line">  loss =</span><br></pre></td></tr></table></figure>
<p>函数<code>backward()</code>中，<code>placeholder()</code>实现对数据集<code>x</code>和标准答案<code>y_</code>占位，<code>forward.forward()</code>实现前向传播的网络结构，参数<code>global_step</code>表示训练轮数，设置为不可训练型参数。在训练网络模型时，常将正则化、指数衰减学习率和滑动平均这三个方法作为模型优化方法。</p>
<ul>
<li>在Tensorflow中，正则化表示为：</li>
</ul>
<p>首先，计算预测结果与标准答案的损失值</p>
<p>(1) <code>MSE</code>: <code>y</code>与<code>y_</code>的差距(loss<em>mse) = `tf.reduce_mean(tf.square(y-y</em>))<code>(2) 交叉熵:</code>ce = tf.nn.sparse<em>softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y</em>,1))<code>`y</code>与<code>y_</code>的差距(cem) = <code>tf.reduce_mean(ce)</code><br>(3) 自定义: <code>y</code>与<code>y_</code>的差距</p>
<p>其次，总损失值为预测结果与标准答案的损失值加上正则化项</p>
<p><code>loss = y 与 y_的差距 + tf.add_n(tf.get_collection(&#39;losses&#39;))</code></p>
<ul>
<li>在Tensorflow中，指数衰减学习率表示为:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    数据集总样本数 / BATCH_SIZE,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase=True)</span><br><span class="line">train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br></pre></td></tr></table></figure>
<ul>
<li>在 Tensorflow 中，滑动平均表示为:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">  train_op = tf.no_op(name=&apos;train&apos;)</span><br></pre></td></tr></table></figure>
<p>其中，滑动平均和指数衰减学习率中的 global_step 为同一个参数。</p>
<ul>
<li>用 with 结构初始化所有参数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  for i in range(STEPS):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x:, y_: &#125;)</span><br><span class="line">    if i % 轮数 == 0:</span><br><span class="line">      print</span><br></pre></td></tr></table></figure>
<p>其中，<code>with</code>结构用于初始化所有参数信息以及实现调用训练过程，并打印出<code>loss</code>值。</p>
<ul>
<li>判断 python 运行文件是否为主文件</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if __name__==&apos;__main__&apos;:</span><br><span class="line">  backward()</span><br></pre></td></tr></table></figure>
<p>该部分用来判断 python 运行的文件是否为主文件。若是主文件，则执行 backword()函数。</p>
<p>例如:用 300 个符合正态分布的点<script type="math/tex">X[x_{0}, x_{1}]</script>作为数据集，根据点<script type="math/tex">X[x_{0}, x_{1}]</script>的不同进行标注<code>Y_</code>，将数据集标注为红色和蓝色。标注规则为:当<script type="math/tex">x_{0} + x_{1} < 2</script>时，<code>y_=1</code>，点 <code>X</code>标注为红色;当<script type="math/tex">x_{0} + x_{1} \geq 2</script>时，<code>y_=0</code>，点<code>X</code>标注为蓝色。我们加入指数衰减学习率优化效率，加入正则化提高泛化性，并使用模块化设计方法，把红色点和蓝色点分开。</p>
<p>代码总共分为三个模块:生成数据集(generateds.py)、前向传播(forward.py)、反向传播 (backward.py)。</p>
<p>(1)生成数据集的模块(generateds.py)</p>
<p>(2)前向传播模块(forward.py)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#0导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入、参数和输出，定义前向传播过程</span><br><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">  w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">  tf.add_to_collection(&apos;losses&apos;,tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">  return w</span><br><span class="line">  </span><br><span class="line">def get_bias(shape):</span><br><span class="line">  b = tf.Variable(tf.constant(0.01, shape=shape))</span><br><span class="line">  return b</span><br><span class="line"></span><br><span class="line">def forward(x, regularizer):</span><br><span class="line">  w1 = get_weight([2,11], regularizer)</span><br><span class="line">  b1 = get_bias([11])</span><br><span class="line">  y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line">  </span><br><span class="line">  w2 = get_weight([11,1], regularizer)</span><br><span class="line">  b2 = get_bias([1])</span><br><span class="line">  y = tf.matmul(y1, w2) + b2 #输出层不过激活</span><br><span class="line">  </span><br><span class="line">  return y</span><br></pre></td></tr></table></figure>
<p>(3) 反向传播模块(backward.py)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#0导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import generateds</span><br><span class="line">import forward</span><br><span class="line"></span><br><span class="line">STEPS = 40000</span><br><span class="line">BATCH_SIZE = 30</span><br><span class="line">LEARNING_RATE_BASE = 0.001</span><br><span class="line">LEARNING_RATE_DECAY = 0.999</span><br><span class="line">REGULARIZER = 0.01</span><br><span class="line"></span><br><span class="line">def backward():</span><br><span class="line">  x = tf.placeholder(tf.float32, shape=(None,2))</span><br><span class="line">  y_ = tf.placeholder(tf.float32, shape=(None,1))</span><br><span class="line">  </span><br><span class="line">  X, Y_, Y_c = generateds.generateds()</span><br><span class="line">  y = forward.forward(x, REGULARIZER)</span><br><span class="line">  global_step = tf.Variable(0, trainable=False)</span><br><span class="line">  learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE, </span><br><span class="line">    global_step,</span><br><span class="line">    300/BATCH_SIZE,</span><br><span class="line">    LEARNING_RATE_DACAY,</span><br><span class="line">    staircase=True)</span><br><span class="line"></span><br><span class="line">  # 定义损失函数</span><br><span class="line">  loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">  loss_total = loss_mse + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line">  </span><br><span class="line">  # 定义反向传播方法：包含正则化</span><br><span class="line">  train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_total)</span><br><span class="line">  with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_varibales_initailizer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">      start = (i*BATCH_SIZE) % 300</span><br><span class="line">      end = start + BATCH_SIZE</span><br><span class="line">      sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">      if i % 2000 == 0:</span><br><span class="line">        loss_v = sess.run(loss_total, feed_dict=&#123;x:X,y_:Y&#125;)</span><br><span class="line">        print(&quot;After %d steps, loss is: %f&quot; % (i, loss_v))</span><br><span class="line">    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    </span><br><span class="line">  plt.scatter(X[:0], X[:,1], c=np.squeeze(Y_c))</span><br><span class="line">  plt.contour(xx, yy, probs, levels=[.5])</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">  backward()</span><br></pre></td></tr></table></figure>
<p>运行代码结果如下：</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328793451699.jpg" width="400"></p>
<p>由运行结果可见，程序使用模块化设计方法，加入指数衰减学习率，使用正则化后，红色点和蓝色点 的分割曲线相对平滑，效果变好。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/16/facenet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/16/facenet/" itemprop="url">一种用于人脸识别和聚类的统一编码</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-16T13:19:36+08:00">
                2018-07-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="原作者："><a href="#原作者：" class="headerlink" title="原作者："></a>原作者：</h2><div class="table-container">
<table>
<thead>
<tr>
<th>作者</th>
<th>邮箱</th>
<th>公司</th>
</tr>
</thead>
<tbody>
<tr>
<td>Florian Schroff</td>
<td>fschroff@google.com</td>
<td>Google Inc.</td>
</tr>
<tr>
<td>Dmitry Kalenichenko</td>
<td>dkalenichenko@google.com</td>
<td>GoogleInc.</td>
</tr>
<tr>
<td>James Philbin</td>
<td>jphilbin@google.com</td>
<td>Google Inc.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>尽管人脸识别领域最近取得了重大进展[10,14,15,17]，但在规模上</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/08/java8-example/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/08/java8-example/" itemprop="url">java8用法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-08T19:07:37+08:00">
                2018-05-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h2 id="Java-Funcational-Interface"><a href="#Java-Funcational-Interface" class="headerlink" title="Java Funcational Interface"></a>Java Funcational Interface</h2><blockquote>
<p>Java提供了<code>@FuncationalInterface</code>注解来创建函数式接口。该注解从Java8开始引入。一个函数式接口必须有一个抽象的方法。函数式接口可以用lambda表达式，方法的引用或者构造器引用来初始化。函数式接口可以拥有一个默认的方法。函数式接口可以通过继承另一个函数式接口创建。Java提供了诸如<code>Supplier</code>,<code>Consumer</code>,<code>Predicate</code>等<code>build-in</code>的函数式接口。</p>
<p>下面我们通过使用<code>@FunctionalInterface</code>注解来创建一个定制的函数式接口。我们会用原型和默认方法以及继承的方式来创建函数式接口。我们还会提供用lamba表达式,方法引用,构造器引用的方式初始化函数式接口。接下来一步步操作。</p>
</blockquote>
<h3 id="FunctionalInterface"><a href="#FunctionalInterface" class="headerlink" title="@FunctionalInterface"></a><code>@FunctionalInterface</code></h3><ol>
<li>用<code>@FunctionalInterface</code>创建函数式接口</li>
<li>一个函数式接口必须有一个抽象的方法</li>
<li>接口中Default的方法不算上面的抽象方法，因为它已经被实现了</li>
<li>如果函数式接口中声明了一个覆盖<code>Object</code>类中<code>public</code>方法的抽象方法，则它也不能算做以上的抽象方法</li>
<li>函数式接口的实例可以通过lambda表达式，方法引用，或者构造器引用来创建</li>
</ol>
<h4 id="创建一个函数式接口"><a href="#创建一个函数式接口" class="headerlink" title="创建一个函数式接口"></a>创建一个函数式接口</h4><p>为了创建定制的函数式接口，需要新建一个用<code>@FuncationalInterface</code>标注的接口类，该接口类必须有一个抽象的方法，抽象方法不需要实现</p>
<p><code>Calculator.java</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.concretepage;</span><br><span class="line"></span><br><span class="line"><span class="meta">@FunctionalInterface</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Calculator</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">long</span> <span class="title">calculate</span><span class="params">(<span class="keyword">long</span> num1, <span class="keyword">long</span> num2)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="用Lambda表达式传参"><a href="#用Lambda表达式传参" class="headerlink" title="用Lambda表达式传参"></a>用Lambda表达式传参</h4><p>语法形式如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(Argument part) -&gt; Body part</span><br></pre></td></tr></table></figure>
<p>现在对函数式接口<code>Calculator</code>进行实例化</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Calculator calc = (n1, n2) -&gt; n1 + n2;</span><br></pre></td></tr></table></figure>
<p>由于抽象方法<code>calculate</code>已经定义了两个<code>long</code>型的数字参数，因此在上面的lambda表达式中，有两个数字型参数。为了得到返回结果，就需要调用函数式的接口。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.out.println(calc.calculate(<span class="number">30</span>, <span class="number">50</span>));</span><br></pre></td></tr></table></figure>
<p>输出结果为 80.</p>
<p><strong>用Lambda表达式通过方法参数传递函数式接口</strong></p>
<p>在某个类<code>MyNumber.java</code>中创建一个包含函数式接口类型的参数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.concretepage;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyNumber</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> firstNum;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> secondNum;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MyNumber</span><span class="params">(<span class="keyword">long</span> firstNum, <span class="keyword">long</span> secondNum)</span> </span>&#123;</span><br><span class="line">	   <span class="keyword">this</span>.firstNum = firstNum;</span><br><span class="line">	   <span class="keyword">this</span>.secondNum = secondNum;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">process</span><span class="params">(Calculator calc)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> calc.calculate(<span class="keyword">this</span>.firstNum, <span class="keyword">this</span>.secondNum);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//setters getters</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样我们可以直接将lambda表达式或函数式接口的实例作为一个参数传递。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;MyNumber&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">list.add(<span class="keyword">new</span> MyNumber(<span class="number">100</span>, <span class="number">40</span>));</span><br><span class="line">list.add(<span class="keyword">new</span> MyNumber(<span class="number">300</span>, <span class="number">60</span>));</span><br><span class="line">list.add(<span class="keyword">new</span> MyNumber(<span class="number">60</span>, <span class="number">20</span>));</span><br></pre></td></tr></table></figure>
<p>我们也可以通过下面的方式使用：</p>
<p><strong>Demo-1：</strong></p>
<p>先用lambda表达式实例化函数式接口，然后作为参数传递给方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Calculator calc = (n1, n2) -&gt; n1 + n2;</span><br><span class="line"><span class="keyword">for</span>(MyNumber myNumber: list) &#123;</span><br><span class="line">   System.out.println(myNumber.process(calc));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">140</span><br><span class="line">360</span><br><span class="line">80</span><br></pre></td></tr></table></figure>
<p><strong>Demo-2:</strong></p>
<p>直接将lambda表达式作为参数传递给方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(MyNumber myNumber: list) &#123;</span><br><span class="line">   System.out.println(myNumber.process((n1, n2) -&gt; n1 * n2));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">4000</span><br><span class="line">18000</span><br><span class="line">1200</span><br></pre></td></tr></table></figure>
<p><strong>Demo-3:</strong></p>
<p>实现除法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(MyNumber myNumber: list) &#123;</span><br><span class="line">   System.out.println(myNumber.process((n1, n2) -&gt; n1 / n2));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">5</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<h4 id="用方法引用实例化函数式接口"><a href="#用方法引用实例化函数式接口" class="headerlink" title="用方法引用实例化函数式接口"></a>用方法引用实例化函数式接口</h4><p>方法引用调用方法使用<code>::</code>标记操作。假设有一个类<code>MyNumber</code>以及它的已经静态方法<code>add</code>,那么我们可以用以下方式调用</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MyNumber::add</span><br></pre></td></tr></table></figure>
<p>如果<code>add</code>不是静态方法,则可以用实例变量来调用。假设<code>myNumber</code>就是<code>MyNumber</code>类的实例变量，并且<code>add</code>是一个非静态的方法，那么就可以用以下方式调用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myNumber::add</span><br></pre></td></tr></table></figure>
<h2 id="Java8-Concat-Streams-Lists-Sets-Arrays-样例"><a href="#Java8-Concat-Streams-Lists-Sets-Arrays-样例" class="headerlink" title="Java8 Concat Streams, Lists, Sets, Arrays 样例"></a>Java8 Concat Streams, Lists, Sets, Arrays 样例</h2><h2 id="Java8-Distinct-样例"><a href="#Java8-Distinct-样例" class="headerlink" title="Java8 Distinct 样例"></a>Java8 Distinct 样例</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/24/microservice/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/24/microservice/" itemprop="url">微服务架构</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-24T13:20:36+08:00">
                2018-04-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="微服务架构定义"><a href="#微服务架构定义" class="headerlink" title="微服务架构定义"></a>微服务架构定义</h2><h3 id="定义一"><a href="#定义一" class="headerlink" title="定义一"></a>定义一</h3><p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-04-24-15245477737178.jpg" alt=""></p>
<ul>
<li>一种架构风格，将单体应用分成一组小的服务，服务之间相互协作，实现业务功能</li>
<li>每个服务运行在独立的进程中，服务间采用轻量级的通信机制协作（通常用<code>HTTP</code>/<code>JSON</code>）</li>
<li>每个服务围绕业务能力进行构建，并且能够通过自动化机制独立部署</li>
<li>很少有集中式的服务管理，每个服务可以使用不同的语言开发，使用不同的存储技术</li>
</ul>
<p>参考: <a href="https://www.martinfowler.com/articles/microservices.html" target="_blank" rel="noopener">https://www.martinfowler.com/articles/microservices.html</a></p>
<h3 id="定义二"><a href="#定义二" class="headerlink" title="定义二"></a>定义二</h3><p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-04-24-15245478650377.jpg" alt=""></p>
<ul>
<li>Loosely coupled service oriented architecture with bounded context</li>
<li>基于有界上下文的，松散耦合的面相服务的架构</li>
</ul>
<h2 id="微服务的利弊"><a href="#微服务的利弊" class="headerlink" title="微服务的利弊"></a>微服务的利弊</h2><div class="table-container">
<table>
<thead>
<tr>
<th>利</th>
<th>弊</th>
</tr>
</thead>
<tbody>
<tr>
<td>强模块化边界</td>
<td>分布式系统复杂性</td>
</tr>
<tr>
<td>可独立部署</td>
<td>最终一致性</td>
</tr>
<tr>
<td>技术多样性</td>
<td>运维复杂性</td>
</tr>
<tr>
<td></td>
<td>测试复杂性</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>如果你搞不定一个单块应用，别指望微服务能够拯救你!</p>
</blockquote>
<h3 id="康威定律（Conway’s-Law）"><a href="#康威定律（Conway’s-Law）" class="headerlink" title="康威定律（Conway’s Law）"></a>康威定律（Conway’s Law）</h3><p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-04-24-15245482569586.jpg" alt=""></p>
<ul>
<li>Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations. - Melvin Conway(1967)</li>
<li>设计系统的组织，其生产的架构等价于组织间的沟通结构</li>
</ul>
<table>
<tr>
<td width="50%"><img src="http://ovhbzkbox.bkt.clouddn.com/2018-04-24-15245484287535.jpg"></td>
<td width="50%"><img src="http://ovhbzkbox.bkt.clouddn.com/2018-04-24-15245484875078.jpg"></td>
</tr>
</table>

<h3 id="微服务的适用性"><a href="#微服务的适用性" class="headerlink" title="微服务的适用性"></a>微服务的适用性</h3><p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-04-24-15245494636586.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/22/Bayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/22/Bayes/" itemprop="url">贝叶斯相关定理及其互联网应用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-22T01:54:37+08:00">
                2017-10-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一、贝叶斯定理-Bayes’-Theorem"><a href="#一、贝叶斯定理-Bayes’-Theorem" class="headerlink" title="一、贝叶斯定理(Bayes’ Theorem)"></a>一、贝叶斯定理(Bayes’ Theorem)</h1><p><img src="http://ovhbzkbox.bkt.clouddn.com/2017-10-22-bayes.png" alt="bayes"></p>
<blockquote>
<p>贝叶斯定理是英国数学家<strong><em>托马斯.贝叶斯</em></strong> (Thomas Bayes)于1763年发表的一片论文中提出的，用来描述两个条件概率之间的关系。</p>
</blockquote>
<h2 id="1-1-条件概率"><a href="#1-1-条件概率" class="headerlink" title="1.1 条件概率"></a>1.1 条件概率</h2><script type="math/tex; mode=display">P(A|B)=\frac{P(AB)}{P(B)}</script><h2 id="1-2-全概率公式"><a href="#1-2-全概率公式" class="headerlink" title="1.2 全概率公式"></a>1.2 全概率公式</h2><script type="math/tex; mode=display">P(A)=\sum_{i}^{}P(A|B_{i})P(B_{i})</script><p><img src="http://ovhbzkbox.bkt.clouddn.com/2017-10-22-condition_prob.png" alt="condition_prob"></p>
<h2 id="1-3-贝叶斯公式"><a href="#1-3-贝叶斯公式" class="headerlink" title="1.3 贝叶斯公式"></a>1.3 贝叶斯公式</h2><script type="math/tex; mode=display">P(B_{i}|A)=\frac{P(A|B_{i})P(B_{i})}{\sum_{j}^{}P(A|B_{j})P(B_{j})}</script><h2 id="1-4-贝叶斯定理"><a href="#1-4-贝叶斯定理" class="headerlink" title="1.4 贝叶斯定理"></a>1.4 贝叶斯定理</h2><p><img src="http://ovhbzkbox.bkt.clouddn.com/2017-10-22-bayes_theom.jpg" alt="bayes_theo"></p>
<h1 id="二、举几个栗子"><a href="#二、举几个栗子" class="headerlink" title="二、举几个栗子"></a>二、举几个栗子</h1><h2 id="2-1-病人分类的栗子"><a href="#2-1-病人分类的栗子" class="headerlink" title="2.1 病人分类的栗子"></a>2.1 病人分类的栗子</h2><p>某个医院早上收了六个门诊病人，如下表</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>症状</th>
<th>职业</th>
<th>疾病</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>打喷嚏</td>
<td>护士</td>
<td>感冒</td>
</tr>
<tr>
<td>2</td>
<td>打喷嚏</td>
<td>农夫</td>
<td>过敏</td>
</tr>
<tr>
<td>3</td>
<td>头痛</td>
<td>建筑工人</td>
<td>脑震荡</td>
</tr>
<tr>
<td>4</td>
<td>头痛</td>
<td>建筑工人</td>
<td>感冒</td>
</tr>
<tr>
<td>5</td>
<td>打喷嚏</td>
<td>教师</td>
<td>感冒</td>
</tr>
<tr>
<td>6</td>
<td>头痛</td>
<td>教师</td>
<td>脑震荡</td>
</tr>
</tbody>
</table>
</div>
<p>现在又来了第七个病人，是一个打喷嚏的建筑工人。问：他患上感冒的概率有多大？</p>
<blockquote>
<p>根据贝叶斯定理</p>
</blockquote>
<script type="math/tex; mode=display">P(A|B)=\frac{P(B|A)P(A)}{P(B)}</script><blockquote>
<p>可得</p>
</blockquote>
<script type="math/tex; mode=display">P(感冒|打喷嚏\times建筑工人)=\frac{P(打喷嚏\times建筑工人|感冒)\times P(感冒)}{P(打喷嚏\times建筑工人)}</script><blockquote>
<p>假设“打喷嚏”和“建筑工人”这两个特征都是独立的，那么以上的等式就等价于</p>
</blockquote>
<script type="math/tex; mode=display">P(感冒|打喷嚏\times建筑工人)=\frac{P(打喷嚏|感冒) \times P(建筑工人|感冒) \times P(感冒)}{P(打喷嚏) \times P(建筑工人)}</script><blockquote>
<p>因此上面的公式可以计算出</p>
</blockquote>
<script type="math/tex; mode=display">P(感冒|打喷嚏\times建筑工人)=\frac{0.66 \times 0.33 \times 0.5}{0.5 \times 0.33} = 0.66</script><blockquote>
<p>因此，这个打喷嚏的建筑工人，有66%的概率是得了感冒。同理，可以计算这个病人患上过敏或脑震荡的概率。比较这几个概率，就可以知道他最可能得什么病。这就是贝叶斯分类器的基本方法：在统计资料的基础上，依据某些特征，计算各个类别的概率，从而实现分类。</p>
</blockquote>
<h2 id="2-2-性别推断的例子"><a href="#2-2-性别推断的例子" class="headerlink" title="2.2 性别推断的例子"></a>2.2 性别推断的例子</h2><p>下面是一组人类身体特征的统计资料</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>性别</th>
<th>身高（英尺）</th>
<th>体重（磅）</th>
<th>脚掌（英寸）</th>
</tr>
</thead>
<tbody>
<tr>
<td>男</td>
<td>6</td>
<td>180</td>
<td>12</td>
</tr>
<tr>
<td>男</td>
<td>5.92</td>
<td>190</td>
<td>11</td>
</tr>
<tr>
<td>男</td>
<td>5.58</td>
<td>170</td>
<td>12</td>
</tr>
<tr>
<td>男</td>
<td>5.92</td>
<td>165</td>
<td>10</td>
</tr>
<tr>
<td>女</td>
<td>5</td>
<td>100</td>
<td>6</td>
</tr>
<tr>
<td>女</td>
<td>5.5</td>
<td>150</td>
<td>8</td>
</tr>
<tr>
<td>女</td>
<td>5.42</td>
<td>130</td>
<td>7</td>
</tr>
<tr>
<td>女</td>
<td>5.75</td>
<td>150</td>
<td>9</td>
</tr>
</tbody>
</table>
</div>
<p>已知某人的身高6英尺、体重130磅，脚掌8英寸，请问该人的性别？</p>
<blockquote>
<p>假设男性和女性的身高、体重、脚掌都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。</p>
<p>比如，男性的身高是均值5.855、方差0.035的正态分布。所以，男性的身高为6英尺的概率的相对值等于1.5789（大于1并没有关系，因为这里是密度函数的值，只用来反映各个值的相对可能性）。</p>
</blockquote>
<script type="math/tex; mode=display">p(height|male)=\frac{1}{\sqrt{2\pi\sigma^2}}exp(\frac{-(6-\mu)^2}{2\sigma^2})\approx1.5789</script><blockquote>
<p>有了这些数据就可以计算性别的分类了。</p>
</blockquote>
<script type="math/tex; mode=display">\boxed{P(身高=6|男)\times P(体重=130|男)\times P(脚掌=8|男)\times P(男)\\ = 6.1984\times e^{-9}\\
P(身高=6|女)\times P(体重=130|女)\times P(脚掌=8|女)\times P(女)\\ = 6.1984\times e^{-4}}</script><blockquote>
<p>由此可以看到，女性的概率比男性要高出约10000倍，所以可以推断该人为女性</p>
</blockquote>
<h1 id="三、互联网应用场景"><a href="#三、互联网应用场景" class="headerlink" title="三、互联网应用场景"></a>三、互联网应用场景</h1><h2 id="3-1-垃圾邮件的过滤"><a href="#3-1-垃圾邮件的过滤" class="headerlink" title="3.1 垃圾邮件的过滤"></a>3.1 垃圾邮件的过滤</h2><h3 id="3-1-1-背景"><a href="#3-1-1-背景" class="headerlink" title="3.1.1 背景"></a>3.1.1 背景</h3><p><img width="400" src="http://ovhbzkbox.bkt.clouddn.com/2017-10-22-spam.png"></p>
<p>垃圾邮件是一种令人头痛的顽症，困扰着所有的互联网用户。<br>正确识别垃圾邮件的技术难度非常大。传统的垃圾邮件过滤方法，主要有”关键词法”和”校验码法”等。前者的过滤依据是特定的词语；后者则是计算邮件文本的校验码，再与已知的垃圾邮件进行对比。它们的识别效果都不理想，而且很容易规避。</p>
<p>2002年，<code>Paul Graham</code>提出使用”贝叶斯推断”过滤垃圾邮件。他说，这样做的效果，好得不可思议。1000封垃圾邮件可以过滤掉995封，且没有一个误判。</p>
<p>另外，这种过滤器还具有自我学习的功能，会根据新收到的邮件，不断调整。收到的垃圾邮件越多，它的准确率就越高。</p>
<h3 id="3-1-2-训练集"><a href="#3-1-2-训练集" class="headerlink" title="3.1.2 训练集"></a>3.1.2 训练集</h3><p>首先我们需要提供两组已经识别好的邮件，一组是正常的邮件，另一组是垃圾邮件。</p>
<p>我们用这两组邮件，对过滤器进行“训练”。这两组邮件的规模越大，训练的效果就越好。<code>Paul Graham</code>使用的邮件规模，是正常邮件和垃圾邮件各4000封。</p>
<p>“训练”过程很简单。首先，解析所有的邮件，提取每一个词。然后计算每个词语在正常邮件和垃圾邮件中出现的频率。</p>
<p>比如，假设“sex”这个词，在4000封垃圾邮件中，有200封包含这个词，那么它出现的频率就是5%；而如果在4000封正常邮件中，只有2封包含这个词，那么出现频率就是0.05%（【注】如果某个词只出现在垃圾邮件中，<code>Paul Graham</code>就假定，它在正常邮件的出现频率是1%，反之亦然。这样做是为了避免概率为0。随着邮件数量的增加，计算结果会自动调整。）<br>有了这个初步的统计结果，过滤器就可以投入使用了。</p>
<h3 id="3-1-2-使用过程"><a href="#3-1-2-使用过程" class="headerlink" title="3.1.2 使用过程"></a>3.1.2 使用过程</h3><p>我们收到了一封新邮件。在未进行统计分析之前，我们假设它是垃圾邮件的概率为50%。（【注】有研究表明，用户收到的电子邮件中，80%是垃圾邮件。但是，这里依然假设垃圾邮件的“先验概率”为50%）</p>
<p>我们用$S$代表垃圾邮件（Spam），$H$表示正常邮件（healthy）。因此$P(S)$和$P(H)$的先验概率都是50%，即<script type="math/tex">P(S)=P(H)=50\%</script></p>
<p>然后，对这封邮件进行解析，发现其中包含了sex这个词，请问这封邮件属于垃圾邮件的概率有多高？</p>
<p>我们用$W$表示“sex”这个词，那么问题就转化为如何计算$P(S|W)$的值，即在某个词语（$W$）已经存在的条件下，垃圾邮件($S$)的概率有多大？</p>
<blockquote>
<p>根据贝叶斯公式，马上可以写出</p>
</blockquote>
<script type="math/tex; mode=display">P(S|W)=\frac{P(W|S)P(S)}{P(W|S)P(S)+P(W|H)P(H)}</script><blockquote>
<p>其中，$P(W|S)$和$P(W|H)$的含义是，这个词语在垃圾邮件和正常邮件中，分别出现的概率。这两个值可以从训练集中得到，对sex这个词来说，上文假定它们分别等于5%和0.05%。另外，$P(S)$和$P(H)$的值，前面假设都等于50%。所以，马上可以计算$P(S|W)$的值。</p>
</blockquote>
<script type="math/tex; mode=display">P(S|W)=\frac{5\%\times50\%}{5\%\times50\%+0.05\%\times50\%}=99.0\%</script><blockquote>
<p>因此，这封新邮件是垃圾邮件的概率等于99%。这说明，sex这个词的推断能力很强，将50%的”先验概率”一下子提高到了99%的”后验概率”。</p>
</blockquote>
<h3 id="3-1-3-问题及改进"><a href="#3-1-3-问题及改进" class="headerlink" title="3.1.3 问题及改进"></a>3.1.3 问题及改进</h3><p>做完上面一步，请问我们能否得出结论，这封新邮件就是垃圾邮件？</p>
<p>回答是不能。因为一封邮件包含很多词语，一些词语（比如sex）说这是垃圾邮件，另一些说这不是。你怎么知道以哪个词为准？</p>
<p><code>Paul Graham</code>的做法是，选出这封信中$P(S|W)$最高的15个词，计算它们的<code>联合概率</code>。（【注】如果有的词是第一次出现，无法计算P(S|W)，Paul Graham就假定这个值等于0.4。因为垃圾邮件用的往往都是某些固定的词语，所以如果你从来没见过某个词，它多半是一个正常的词。）</p>
<p>所谓<code>联合概率</code>，就是指在多个事件发生的情况下，另一个事件发生概率有多大。比如，已知W1和W2是两个不同的词语，它们都出现在某封电子邮件之中，那么这封邮件是垃圾邮件的概率，就是<code>联合概率</code>。</p>
<p>在已知$W1$和$W2$的情况下，无非就是两种结果：垃圾邮件（事件$E1$）或正常邮件（事件$E2$）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事件</th>
<th>W1</th>
<th>W2</th>
<th>是否垃圾邮件</th>
</tr>
</thead>
<tbody>
<tr>
<td>E1</td>
<td>出现</td>
<td>出现</td>
<td>是的</td>
</tr>
<tr>
<td>E2</td>
<td>出现</td>
<td>出现</td>
<td>不是</td>
</tr>
</tbody>
</table>
</div>
<p>其中，$W1$和$W2$和垃圾邮件的概率分别如下</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事件</th>
<th>W1</th>
<th>W2</th>
<th>垃圾邮件</th>
</tr>
</thead>
<tbody>
<tr>
<td>E1</td>
<td>P(S\</td>
<td>W1)</td>
<td>P(S\</td>
<td>W2)</td>
<td>P(S)</td>
</tr>
<tr>
<td>E2</td>
<td>1-P(S\</td>
<td>W1)</td>
<td>1-P(S\</td>
<td>W2)</td>
<td>1-P(S)</td>
</tr>
</tbody>
</table>
</div>
<p>如果假定所有事件都是独立事件（【注】严格地说，这个假定不成立，但是这里可以忽略），那么就可以计算P(E1)和P(E2)：</p>
<script type="math/tex; mode=display">
P(E1)=P(S|W1)P(S|W2)P(S)
\\
P(E2)=(1-P(S|W1))(1-P(S|W2))(1-P(S))</script><p>又由于在$W1$和$W2$已经发生的情况下，垃圾邮件的概率等于下面的式子：</p>
<script type="math/tex; mode=display">P=\frac{P(E1)}{P(E1)+P(E2)}</script><p>即</p>
<script type="math/tex; mode=display">P=\frac{P(S|W1)P(S|W2)P(S)}{P(S|W1)P(S|W2)P(S)+(1-P(S|W1))(1-P(S|W2))(1-P(S))}</script><p>将$P(S|W1)=0.5$代入，得到</p>
<script type="math/tex; mode=display">P=\frac{P(S|w1)P(S|W2)}{P(S|W1)P(S|W2)+(1-P(S|W1))(1-P(S|W2))}</script><p>将$P(S|W1)$记为$P1$，$P(S|W2)$记为$P2$，公式就变为</p>
<script type="math/tex; mode=display">P=\frac{P1P2}{P1P2+(1-P1)(1-P2)}</script><p>就就是<code>联合概率</code>的计算公式。</p>
<p>将上面的公式扩展到15个词的情况，就得到了最终的概率计算公式:</p>
<script type="math/tex; mode=display">P=\frac{P_1P_2\dots P_{15}}{P_1P_2\dots P_{15}+(1-P_1)(1-P_2)\dots (1-P_{15})}</script><p>一封邮件是不是垃圾邮件，就用这个式子进行计算。这时我们还需要一个用于比较的门槛值。Paul Graham的门槛值是0.9，概率大于0.9，表示15个词联合认定，这封邮件有90%以上的可能属于垃圾邮件；概率小于0.9，就表示是正常邮件。</p>
<p>有了这个公式以后，一封正常的信件即使出现sex这个词，也不会被认定为垃圾邮件了。</p>
<h2 id="3-2-单词拼写检查"><a href="#3-2-单词拼写检查" class="headerlink" title="3.2 单词拼写检查"></a>3.2 单词拼写检查</h2><h3 id="3-2-1-场景"><a href="#3-2-1-场景" class="headerlink" title="3.2.1 场景"></a>3.2.1 场景</h3><p>当你在使用搜索引擎的时候，如果你拼错了一个单词，它会提示你正确的拼法。比如你不小心输入ehllo</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2017-10-22-baidu.png" alt="baidu"></p>
<p>搜索引擎会提醒你：您要找的是不是hello，你可以意识到正确的拼法应该是hello</p>
<p>这就叫做“拼写检查”(spelling correct)。有好几种方法可以实现这个功能，Google使用的是基于贝叶斯推断的统计学方法。这种方法的特点是快，在很短的时间内处理大量的文本，并且有很高的精确度（90%以上）。Google的研发总监Peter Norvig写过一篇著名的文章，解释过这种原理。（【注】<a href="http://norvig.com/spell-correct.html）" target="_blank" rel="noopener">http://norvig.com/spell-correct.html）</a></p>
<p>下面我们就来看看，怎么利用贝叶斯推断，实现”拼写检查”。其实很简单，一小段代码就够了。</p>
<h3 id="3-2-2-原理"><a href="#3-2-2-原理" class="headerlink" title="3.2.2 原理"></a>3.2.2 原理</h3><p>用户输入了一个单词。这时分成两种情况：拼写正确，或者拼写不正确。我们把拼写正确的情况记做$c$（代表correct），拼写错误的情况记做$w$（代表wrong）。</p>
<p>所谓”拼写检查”，就是在发生$w$的情况下，试图推断出$c$。从概率论的角度看，就是已知$w$，然后在若干个备选方案中，找出可能性最大的那个$c$，也就是求下面这个式子的最大值。</p>
<script type="math/tex; mode=display">P(c|w)</script><p>根据贝叶斯定理</p>
<script type="math/tex; mode=display">P(c|w)=\frac{P(w|c)P(c)}{P(w)}</script><p>对于所有备选的c来说，对应的都是同一个w，所以它们的P(w)是相同的，因此我们求的其实是$P(w|c)P(c)$的最大值。</p>
<p>$P(c)$的含义是，某个正确的词的出现”概率”，它可以用”频率”代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，$P(c)$就越大。</p>
<p>$P(w|c)$的含义是，在试图拼写$c$的情况下，出现拼写错误$w$的概率。这需要统计数据的支持，但是为了简化问题，我们假设两个单词在字形上越接近，就有越可能拼错，$P(w|C)$就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词<code>hello</code>，那么错误拼成<code>hallo</code>（相差一个字母）的可能性，就比拼成<code>haallo</code>高（相差两个字母）。</p>
<p>所以，我们只要找到与输入单词在字形上最相近的那些词，再在其中挑出出现频率最高的一个，就能实现 $P(w|c)P(c)$ 的最大值。</p>
<h3 id="3-2-3-算法"><a href="#3-2-3-算法" class="headerlink" title="3.2.3 算法"></a>3.2.3 算法</h3><p>最简单的算法，只需要四步就够了。</p>
<p><strong>第一步，建立一个足够大的文本库。</strong></p>
<p><strong>第二步，取出文本库的每一个单词，统计它们的出现频率。</strong></p>
<p><strong>第三步，根据用户输入的单词，得到其所有可能的拼写相近的形式。</strong></p>
<blockquote>
<p>所谓”拼写相近”，指的是两个单词之间的”编辑距离”（edit distance）不超过2。也就是说，两个词只相差1到2个字母，只通过——删除、交换、更改和插入——这四种操作中的一种，就可以让一个词变成另一个词。</p>
</blockquote>
<p><strong>第四步，比较所有拼写相近的词在文本库的出现频率。频率最高的那个词，就是正确的拼法。</strong></p>
<blockquote>
<p>根据<code>Peter Norvig</code>的验证，这种算法的精确度大约为60%-70%（10个拼写错误能够检查出6个。）虽然不令人满意，但是能够接受。毕竟它足够简单，计算速度极快。（本文的最后部分，将详细讨论这种算法的缺陷在哪里。）</p>
</blockquote>
<h3 id="3-2-3-Talk-is-Cheap，Show-me-the-code"><a href="#3-2-3-Talk-is-Cheap，Show-me-the-code" class="headerlink" title="3.2.3 Talk is Cheap，Show me the code"></a>3.2.3 Talk is Cheap，Show me the code</h3><h3 id="3-2-4-缺陷"><a href="#3-2-4-缺陷" class="headerlink" title="3.2.4 缺陷"></a>3.2.4 缺陷</h3><p>我们使用的这种算法，有一些缺陷，如果投入生产环境，必须在这些方面加入改进。</p>
<p><strong>1. 文本库必须有很高的精确性，不能包含拼写错误的词。</strong></p>
<blockquote>
<p>如果用户输入一个错误的拼法，文本库恰好包含了这种拼法，它就会被当成正确的拼法。</p>
</blockquote>
<p><strong>2. 对于不包含在文本库中的新词，没有提出解决办法。</strong></p>
<blockquote>
<p>如果用户输入一个新词，这个词不在文本库之中，就会被当作错误的拼写进行纠正。</p>
</blockquote>
<p><strong>3. 程序返回的是”编辑距离”为1的词，但某些情况下，正确的词的”编辑距离”为2。</strong></p>
<blockquote>
<p>比如，用户输入reciet，会被纠正为recite（编辑距离为1）,但用户真正想要输入的词是receipt（编辑距离为2）。也就是说，”编辑距离”越短越正确的规则，并非所有情况下都成立。</p>
</blockquote>
<p><strong>4. 有些常见拼写错误的”编辑距离”大于2。</strong></p>
<blockquote>
<p>这样的错误，程序无法发现。下面就是一些例子，每一行前面那个词是正确的拼法，后面那个则是常见的错误拼法。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">purple perpul</span><br><span class="line">curtains courtens</span><br><span class="line">minutes muinets</span><br><span class="line">successful sucssuful</span><br><span class="line">inefficient ineffiect</span><br><span class="line">availability avaiblity</span><br><span class="line">dissension desention</span><br><span class="line">unnecessarily unessasarily</span><br><span class="line">necessary nessasary</span><br><span class="line">unnecessary unessessay</span><br><span class="line">night nite</span><br><span class="line">assessing accesing</span><br><span class="line">necessitates nessisitates</span><br></pre></td></tr></table></figure>
<p><strong>5. 用户输入的词的拼写正确，但是其实想输入的是另一个词。</strong></p>
<blockquote>
<p>比如，用户输入是where，这个词拼写正确，程序不会纠正。但是，用户真正想输入的其实是were，不小心多打了一个h。</p>
</blockquote>
<p><strong>6. 程序返回的是出现频率最高的词，但用户真正想输入的是另一个词。</strong></p>
<blockquote>
<p>比如，用户输入ther，程序会返回the，因为它的出现频率最高。但是，用户真正想输入的其实是their，少打了一个i。也就是说，出现频率最高的词，不一定就是用户想输入的词。</p>
</blockquote>
<p><strong>7. 某些词有不同的拼法，程序无法辨别。</strong></p>
<blockquote>
<p>比如，英国英语和美国英语的拼法不一致。英国用户输入’humur’，应该被纠正为’humour’；美国用户输入’humur’，应该被纠正为’humor’。但是，我们的程序会统一纠正为’humor’。</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/17/mac-dev/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/17/mac-dev/" itemprop="url">Mac下开发环境搭建</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-17T17:02:07+08:00">
                2017-10-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p>工欲善其事，必先利其器</p>
</blockquote>
<h4 id="通过AppStore安装Xcode"><a href="#通过AppStore安装Xcode" class="headerlink" title="通过AppStore安装Xcode"></a>通过AppStore安装Xcode</h4><h5 id="安装完毕后请在shell命令行中执行如下命令安装Xcode的命令行工具："><a href="#安装完毕后请在shell命令行中执行如下命令安装Xcode的命令行工具：" class="headerlink" title="安装完毕后请在shell命令行中执行如下命令安装Xcode的命令行工具："></a>安装完毕后请在<code>shell</code>命令行中执行如下命令安装Xcode的命令行工具：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xcode-select --install</span><br></pre></td></tr></table></figure>
<h5 id="修改sudo权限（方便以后执行sudo命令时不需要输入密码）"><a href="#修改sudo权限（方便以后执行sudo命令时不需要输入密码）" class="headerlink" title="修改sudo权限（方便以后执行sudo命令时不需要输入密码）"></a>修改sudo权限（方便以后执行sudo命令时不需要输入密码）</h5><p>请在<code>shell</code>中执行<code>sudo visudo</code>，修改以下部分内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">%</span>admin ALL = (ALL) NOPASSWD: ALL</span><br></pre></td></tr></table></figure>
<h4 id="安装命令行工具"><a href="#安装命令行工具" class="headerlink" title="安装命令行工具"></a>安装命令行工具</h4><h5 id="安装Homebrew"><a href="#安装Homebrew" class="headerlink" title="安装Homebrew"></a>安装<code>Homebrew</code></h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 安装 homebrew</span><br><span class="line">/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 添加 homebrew tap</span><br><span class="line">brew tap caskroom/cask</span><br><span class="line">brew tap caskroom/versions</span><br></pre></td></tr></table></figure>
<h5 id="安装zsh相关"><a href="#安装zsh相关" class="headerlink" title="安装zsh相关"></a>安装zsh相关</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew install zsh zsh-completions</span><br><span class="line">sh -c "$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)"</span><br></pre></td></tr></table></figure>
<h5 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> JDK 8 必须安装</span><br><span class="line">brew cask install java8</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> JDK 9 可选安装</span><br><span class="line">brew cask install java</span><br></pre></td></tr></table></figure>
<p>如果想要安装多个版本的JDK，可以使用<code>jenv</code>来管理：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 安装并设置 jenv</span><br><span class="line">brew install jenv</span><br><span class="line">echo 'export PATH="$HOME/.jenv/bin:$PATH"' &gt;&gt; ~/.zshrc</span><br><span class="line">echo 'eval "$(jenv init -)"' &gt;&gt; ~/.zshrc</span><br></pre></td></tr></table></figure>
<p>安装 jenv 之后，重新开启一个 shell 窗口来设置 JDK，例如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 添加多个 JDK 到 jenv</span><br><span class="line">jenv add /Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home</span><br><span class="line">jenv add /Library/Java/JavaVirtualMachines/jdk-9.jdk/Contents/Home</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 开启 export 插件</span><br><span class="line">jenv enable-plugin export</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 设置全局 JDK 为 JDK 8</span><br><span class="line">jenv global 1.8</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 查看当前 JDK 版本</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure>
<h5 id="安装Maven"><a href="#安装Maven" class="headerlink" title="安装Maven"></a>安装Maven</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install maven</span><br></pre></td></tr></table></figure>
<h5 id="安装其他命令行工具"><a href="#安装其他命令行工具" class="headerlink" title="安装其他命令行工具"></a>安装其他命令行工具</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install autojump wget git diff-so-fancy</span><br></pre></td></tr></table></figure>
<h5 id="配置zsh"><a href="#配置zsh" class="headerlink" title="配置zsh"></a>配置zsh</h5><p>编辑~/.zshrc，内容如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span># 修改 plugins 部分的代码如下</span><br><span class="line"></span><br><span class="line">plugins=(osx autojump git)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span># 在文件末尾添加以下代码</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Config zsh completions</span><br><span class="line">fpath=(/usr/local/share/zsh-completions $fpath)</span><br><span class="line"><span class="meta">#</span> Config autojump</span><br><span class="line">[ -f /usr/local/etc/profile.d/autojump.sh ] &amp;&amp; . /usr/local/etc/profile.d/autojump.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Config JAVA HOME</span><br><span class="line">export JAVA_HOME=`/usr/libexec/java_home`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Config NVM for Node.js</span><br><span class="line">export NVM_HOME="$HOME/.nvm"</span><br><span class="line">export NVM_NODEJS_ORG_MIRROR=https://npm.taobao.org/mirrors/node</span><br><span class="line">export PHANTOMJS_CDNURL=http://npm.taobao.org/mirrors/phantomjs</span><br><span class="line">export SELENIUM_CDNURL=http://npm.taobao.org/mirrorss/selenium</span><br><span class="line">export CHROMEDRIVER_CDNURL=http://npm.taobao.org/mirrors/chromedriver</span><br><span class="line">[ -s "$NVM_HOME/nvm.sh" ] &amp;&amp; . "$NVM_HOME/nvm.sh"</span><br></pre></td></tr></table></figure>
<h5 id="配置git"><a href="#配置git" class="headerlink" title="配置git"></a>配置git</h5><p>创建~/.gitconfig，内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">[user]</span><br><span class="line">    name = &lt;nickname&gt;</span><br><span class="line">    email = &lt;nickname@wacai.com&gt;</span><br><span class="line"></span><br><span class="line">[alias]</span><br><span class="line">    ci = commit -m</span><br><span class="line">    co = checkout</span><br><span class="line">    st = status</span><br><span class="line">    br = branch</span><br><span class="line">    wc = whatchanged</span><br><span class="line">    df = diff</span><br><span class="line">    up = pull --rebase --autostash --all</span><br><span class="line">    pom = push origin master</span><br><span class="line">    unstage = reset HEAD</span><br><span class="line">    last = log -1 HEAD</span><br><span class="line">    sl = shortlog -s</span><br><span class="line">    lg = log -p</span><br><span class="line">    gl = log --all --graph --pretty=format:'%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit --date=relative</span><br><span class="line">    praise = blame</span><br><span class="line"></span><br><span class="line">[core]</span><br><span class="line">    autocrlf = &lt;macOS 配置为 input，Windows 配置为 true&gt;</span><br><span class="line">    quotepath = false</span><br><span class="line">    ignorecase = false</span><br><span class="line">    pager = diff-so-fancy | less --tabs=4 -RFX</span><br><span class="line">    whitespace = trailing-space,space-before-tab,indent-with-non-tab</span><br><span class="line">    editor = /usr/bin/vim</span><br><span class="line"></span><br><span class="line">[merge]</span><br><span class="line">    ff = false</span><br><span class="line"></span><br><span class="line">[pull]</span><br><span class="line">    rebase = true</span><br><span class="line"></span><br><span class="line">[push]</span><br><span class="line">    default = simple</span><br><span class="line"></span><br><span class="line">[color]</span><br><span class="line">    status = auto</span><br><span class="line">    branch = auto</span><br><span class="line">    diff = auto</span><br><span class="line">    ui = true</span><br><span class="line">    pager = true</span><br><span class="line"></span><br><span class="line">[color "branch"]</span><br><span class="line">    current = green reverse</span><br><span class="line">    local = yellow</span><br><span class="line">    remote = red</span><br><span class="line"></span><br><span class="line">[color "status"]</span><br><span class="line">    added = yellow bold</span><br><span class="line">    changed = red bold</span><br><span class="line">    untracked = white bold</span><br><span class="line"></span><br><span class="line">[color "diff"]</span><br><span class="line">    meta = yellow bold</span><br><span class="line">    frag = magenta bold</span><br><span class="line">    old = red bold</span><br><span class="line">    new = green bold</span><br><span class="line"></span><br><span class="line">[color "diff-highlight"]</span><br><span class="line">    oldNormal = red bold</span><br><span class="line">    oldHighlight = red bold 52</span><br><span class="line">    newNormal = green bold</span><br><span class="line">    newHighlight = green bold 22</span><br></pre></td></tr></table></figure>
<h5 id="安装Node相关工具"><a href="#安装Node相关工具" class="headerlink" title="安装Node相关工具"></a>安装Node相关工具</h5><p>先创建~/.npmrc，内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">proxy=null</span><br><span class="line">registry=https://registry.npm.taobao.org</span><br><span class="line">disturl=https://npm.taobao.org/dist</span><br></pre></td></tr></table></figure>
<p>然后安装 nvm 和 Node。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.4/install.sh | bash</span><br><span class="line">nvm install node</span><br></pre></td></tr></table></figure>
<h5 id="安装其他中间件"><a href="#安装其他中间件" class="headerlink" title="安装其他中间件"></a>安装其他中间件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install mysql mongodb nginx jenkins zookeeper redis tomcat</span><br></pre></td></tr></table></figure>
<p>安装launchrocket</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew cask install launchrocket</span><br></pre></td></tr></table></figure>
<p>安装完launchrocket之后，可以在Mac的系统偏好设置中找到，可以通过界面来操作所有中间件的启停<br><img src="http://ovhbzkbox.bkt.clouddn.com/2017-10-17-launchrocket.png" alt="launchrocket"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/25/nodejs-promise/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/25/nodejs-promise/" itemprop="url">ECMAScript6异步操作</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-25T14:05:43+08:00">
                2017-08-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p>前段时间有幸看到青哥写的Nodejs代码的影响，据说这货在默认的情况下性能很牛逼，主要在于nodejs默认都是单线程异步执行的。作为一个长时间写Java代码的人来说，直接写node还真不习惯，主要不习惯的一点就是，如果你要在node里写一段顺序执行的串行代码，可能会踩坑。废话不多说了，关于ES6的理论和规范可以参考<code>阮一峰</code>的《ECMAScript 6 入门》，<a href="http://es6.ruanyifeng.com/" target="_blank" rel="noopener">http://es6.ruanyifeng.com/</a></p>
</blockquote>
<h2 id="Demo-1"><a href="#Demo-1" class="headerlink" title="Demo 1"></a>Demo 1</h2><h3 id="要求每隔一秒输出一个累加的算式"><a href="#要求每隔一秒输出一个累加的算式" class="headerlink" title="要求每隔一秒输出一个累加的算式"></a>要求每隔一秒输出一个累加的算式</h3><blockquote>
<p>计算1+2+3+4+5=15</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第一秒: 1+2=3</span><br><span class="line">第二秒: 3+3=6</span><br><span class="line">第三秒: 6+4=10</span><br><span class="line">第四秒: 10+5=15</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>解法一：使用then语法</strong></p>
</blockquote>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 延迟timeout毫秒，执行x1+x2，并输出算式</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">add</span>(<span class="params">x1,x2,timeout</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>) =&gt;</span> &#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span>=&gt;</span>&#123;</span><br><span class="line">      result = x1 + x2;</span><br><span class="line">      <span class="built_in">console</span>.log(x1 + <span class="string">"+"</span> + x2 + <span class="string">"="</span> + result);</span><br><span class="line">      resolve(result);</span><br><span class="line">    &#125;,timeout);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用then的链式语法输出</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">test_1</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">"then语法链式输出========"</span>);</span><br><span class="line">  add(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1000</span>).then(<span class="function">(<span class="params">x</span>)=&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> add(x,<span class="number">3</span>,<span class="number">1000</span>)</span><br><span class="line">  &#125;).then(<span class="function">(<span class="params">x</span>)=&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> add(x,<span class="number">4</span>,<span class="number">1000</span>)</span><br><span class="line">  &#125;).then(<span class="function">(<span class="params">x</span>)=&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> add(x,<span class="number">5</span>,<span class="number">1000</span>)</span><br><span class="line">  &#125;).then(<span class="function">(<span class="params">x</span>)=&gt;</span>&#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span>=&gt;</span>&#123;</span><br><span class="line">      <span class="built_in">console</span>.log(<span class="string">"链式语法输出:1+2+3+4+5="</span> + x + <span class="string">"\n\n"</span>);</span><br><span class="line">    &#125;,<span class="number">1000</span>);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//执行测试</span></span><br><span class="line">test_1();</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>解法二：使用协程（coroutine）</strong></p>
</blockquote>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> co = <span class="built_in">require</span>(<span class="string">'co'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 延迟timeout毫秒，执行x1+x2，并输出算式</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">add</span>(<span class="params">x1,x2,timeout</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>) =&gt;</span> &#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span>=&gt;</span>&#123;</span><br><span class="line">      result = x1 + x2;</span><br><span class="line">      <span class="built_in">console</span>.log(x1 + <span class="string">"+"</span> + x2 + <span class="string">"="</span> + result);</span><br><span class="line">      resolve(result);</span><br><span class="line">    &#125;,timeout);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用Generator函数实现协程</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">function</span>* <span class="title">g</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> x = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">let</span> i = <span class="number">2</span>; i &lt;= <span class="number">5</span>; i++) &#123;</span><br><span class="line">    x = <span class="keyword">yield</span> add(x,i,<span class="number">1000</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用co组件调度协程,并获取最后的结果输出</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">test_2</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  co(g).then(<span class="function">(<span class="params">x</span>)=&gt;</span>&#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span>=&gt;</span>&#123;</span><br><span class="line">      <span class="built_in">console</span>.log(<span class="string">"协程语法输出:1+2+3+4+5="</span> + x + <span class="string">"\n\n"</span>);</span><br><span class="line">    &#125;,<span class="number">1000</span>);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//执行测试</span></span><br><span class="line">test_2();</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>解法三: 使用async函数</strong><br>需要补充说明的是async函数其实就是解法二的语法糖，async相当于Generator函数的星号(*),await则相当于yield，仅此而已。</p>
</blockquote>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 延迟timeout毫秒，执行x1+x2，并输出算式</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">add</span>(<span class="params">x1,x2,timeout</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>) =&gt;</span> &#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span>=&gt;</span>&#123;</span><br><span class="line">      result = x1 + x2;</span><br><span class="line">      <span class="built_in">console</span>.log(x1 + <span class="string">"+"</span> + x2 + <span class="string">"="</span> + result);</span><br><span class="line">      resolve(result);</span><br><span class="line">    &#125;,timeout);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * async方式实现协程</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">function</span> <span class="title">g</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">let</span> x = <span class="number">1</span>;</span><br><span class="line">  x = <span class="keyword">await</span> add(x,<span class="number">2</span>,<span class="number">1000</span>);</span><br><span class="line">  x = <span class="keyword">await</span> add(x,<span class="number">3</span>,<span class="number">1000</span>);</span><br><span class="line">  x = <span class="keyword">await</span> add(x,<span class="number">4</span>,<span class="number">1000</span>);</span><br><span class="line">  x = <span class="keyword">await</span> add(x,<span class="number">5</span>,<span class="number">1000</span>);</span><br><span class="line">  <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 执行async协程，并获取最终的结果</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">test_3</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  g().then(<span class="function">(<span class="params">x</span>)=&gt;</span>&#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span>=&gt;</span>&#123;</span><br><span class="line">      <span class="built_in">console</span>.log(<span class="string">"aync await语法输出:1+2+3+4+5="</span> + x + <span class="string">"\n\n"</span>);</span><br><span class="line">    &#125;,<span class="number">1000</span>);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//执行测试</span></span><br><span class="line">test_3();</span><br></pre></td></tr></table></figure>
<p>以上这三种方式都是等价的</p>
<h2 id="Demo-2"><a href="#Demo-2" class="headerlink" title="Demo 2"></a>Demo 2</h2><h3 id="基于Demo1中的add方法计算1-2-3-…-100-5050"><a href="#基于Demo1中的add方法计算1-2-3-…-100-5050" class="headerlink" title="基于Demo1中的add方法计算1+2+3+…+100=5050"></a>基于Demo1中的add方法计算1+2+3+…+100=5050</h3><blockquote>
<p>要求分10个异步任务执行，这类似于Java中多线程CountDownLatch的调度方式，主线程需要等待所有子线程执行完毕后，获取其结果进行计算的场景。<br>Task1: y1 = 1+2+…+10<br>Task2: y2 = 11+12+…+20<br>Task3: y3 = 21+22+…+30<br>Task4: y4 = 31+32+…+40<br>Task5: y5 = 41+42+…+50<br>Task6: y6 = 51+52+…+60<br>Task7: y7 = 61+62+…+70<br>Task8: y8 = 71+72+…+80<br>Task9: y9 = 81+82+…+90<br>Task10: y10 = 91+92+…+100<br>并行执行这10个任务，并在最后将y1+y2+…+y10求和计算出；由于add函数每次执行加法需要1秒，如果串行的执行需要100秒，那么如果利用并行机制可以可控制在10秒将1到100的和求出！</p>
</blockquote>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 延迟timeout毫秒，执行x1+x2，并输出算式</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">add</span>(<span class="params">x1,x2,timeout</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>) =&gt;</span> &#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span>=&gt;</span>&#123;</span><br><span class="line">      result = x1 + x2;</span><br><span class="line">      <span class="built_in">console</span>.log(x1 + <span class="string">"+"</span> + x2 + <span class="string">"="</span> + result);</span><br><span class="line">      resolve(result);</span><br><span class="line">    &#125;,timeout);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 求1+2+3+...+100 = ?</span></span><br><span class="line"><span class="comment"> * 使用多个协程并发执行</span></span><br><span class="line"><span class="comment"> * 协程1 1+2+..+10 = x</span></span><br><span class="line"><span class="comment"> * 协程2 11+12+..+20 = y</span></span><br><span class="line"><span class="comment"> * ....</span></span><br><span class="line"><span class="comment"> * 最后汇总 x+y+...=z</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">function</span> <span class="title">sum</span>(<span class="params">begin,end</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">let</span> x = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">let</span> i = begin; i &lt;= end; i++) &#123;</span><br><span class="line">    x = <span class="keyword">await</span> add(x,i,<span class="number">1000</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">Promise</span>.all([sum(<span class="number">1</span>,<span class="number">10</span>),sum(<span class="number">11</span>,<span class="number">20</span>),sum(<span class="number">21</span>,<span class="number">30</span>),sum(<span class="number">31</span>,<span class="number">40</span>),sum(<span class="number">41</span>,<span class="number">50</span>),sum(<span class="number">51</span>,<span class="number">60</span>),sum(<span class="number">61</span>,<span class="number">70</span>),sum(<span class="number">71</span>,<span class="number">80</span>),sum(<span class="number">81</span>,<span class="number">90</span>),sum(<span class="number">91</span>,<span class="number">100</span>)]).then(<span class="function"><span class="params">result</span>=&gt;</span>&#123;</span><br><span class="line">   <span class="keyword">return</span> result.reduce(<span class="function">(<span class="params">x,y</span>) =&gt;</span> &#123; <span class="keyword">return</span> x+y;&#125;);</span><br><span class="line">&#125;).then(<span class="function"><span class="params">x</span> =&gt;</span> &#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">"1+2+...+100="</span>+x);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/11/git-common-command/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/11/git-common-command/" itemprop="url">Git常用命令速记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-11T17:48:19+08:00">
                2017-08-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>目前我每天都在使用Git维护代码，但是长期使用smartgit之类的客户端工具，许多常用的命令自然就记不住了。一般来说，日常使用只要记住下图所示的6个命令就可以了。但是如果要熟练使用，恐怕要记住60-1000个命令了。<br><img src="http://oueonj87a.bkt.clouddn.com/0DA07560-CCD2-4942-AA4A-1E38475ACE75.png" alt="0DA07560-CCD2-4942-AA4A-1E38475ACE75"><br>下面是常用的Git命令清单。几个名词译名如下：</p>
<ol>
<li>Workspace: 工作区域</li>
<li>Index/Stage: 暂存区</li>
<li>Repository: 仓库区（或本地仓库）</li>
<li>Remote: 远程仓库</li>
</ol>
</blockquote>
<h2 id="一、新建代码库"><a href="#一、新建代码库" class="headerlink" title="一、新建代码库"></a>一、新建代码库</h2><blockquote>
<p>在当前目录创建一个Git代码库</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br></pre></td></tr></table></figure>
<blockquote>
<p>新建一个目录，将其初始化为Git代码库</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git init [project-name]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>下载一个项目和它的整个代码历史</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone [url]</span><br></pre></td></tr></table></figure>
<h2 id="二、配置"><a href="#二、配置" class="headerlink" title="二、配置"></a>二、配置</h2><blockquote>
<p>Git的配置文件为<code>.gitconfig</code>，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）<br>显示当前的Git配置</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git config --list</span><br></pre></td></tr></table></figure>
<blockquote>
<p>编辑Git配置文件</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git config -e [--global]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>设置提交代码时的用户信息</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git config [--global] user.name &quot;[name]&quot;</span><br><span class="line">$ git config [--global] user.email &quot;[email address]&quot;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示当前的Git配置</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git config --list</span><br></pre></td></tr></table></figure>
<h2 id="三、增加-删除文件"><a href="#三、增加-删除文件" class="headerlink" title="三、增加/删除文件"></a>三、增加/删除文件</h2><blockquote>
<p>添加指定文件到暂存区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git add [file1] [file2] ...</span><br></pre></td></tr></table></figure>
<blockquote>
<p>添加指定目录到暂存区，包括子目录</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git add [dir]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>添加当前目录的所有文件到暂存区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git add .</span><br></pre></td></tr></table></figure>
<blockquote>
<p>添加每个变化前，都会要求确认<br>对于同一个文件的多处变化，可以实现分次提交</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git add -p</span><br></pre></td></tr></table></figure>
<blockquote>
<p>删除工作区文件，并且将这次删除放入暂存区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git rm [file1] [file2] ...</span><br></pre></td></tr></table></figure>
<blockquote>
<p>停止追踪指定文件，但该文件会保留在工作区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git rm --cached [file]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>改名文件，并且将这个改名放入暂存区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git mv [file-original] [file-renamed]</span><br></pre></td></tr></table></figure>
<h2 id="四、代码提交"><a href="#四、代码提交" class="headerlink" title="四、代码提交"></a>四、代码提交</h2><blockquote>
<p>提交暂存区到仓库区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -m [message]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>提交暂存区的指定文件到仓库区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git commit [file1] [file2] ... -m [message]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>提交工作区自上次commit之后的变化，直接到仓库区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -a</span><br></pre></td></tr></table></figure>
<blockquote>
<p>提交时显示所有diff信息</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -v</span><br></pre></td></tr></table></figure>
<blockquote>
<p>使用一次新的commit，替代上一次提交<br>如果代码没有任何新变化，则用来改写上一次commit的提交信息</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git commit --amend -m [message]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>重做上一次commit，并包括指定文件的新变化</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git commit --amend [file1] [file2] ...</span><br></pre></td></tr></table></figure>
<h2 id="五、分支"><a href="#五、分支" class="headerlink" title="五、分支"></a>五、分支</h2><blockquote>
<p>列出所有本地分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git branch</span><br></pre></td></tr></table></figure>
<blockquote>
<p>列出所有远程分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git branch -r</span><br></pre></td></tr></table></figure>
<blockquote>
<p>列出所有本地分支和远程分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git branch -a</span><br></pre></td></tr></table></figure>
<blockquote>
<p>新建一个分支，但依然停留在当前分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git branch [branch-name]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>新建一个分支，并切换到该分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -b [branch]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>新建一个分支，指向指定commit</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git branch [branch] [commit]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>新建一个分支，与指定的远程分支建立追踪关系</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git branch --track [branch] [remote-branch]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>切换到指定分支，并更新工作区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout [branch-name]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>切换到上一个分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -</span><br></pre></td></tr></table></figure>
<blockquote>
<p>建立追踪关系，在现有分支与指定的远程分支之间</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git branch --set-upstream [branch] [remote-branch]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>合并指定分支到当前分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git merge [branch]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>选择一个commit，合并进当前分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git cherry-pick [commit]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>删除分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git branch -d [branch-name]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>删除远程分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin --delete [branch-name]</span><br><span class="line">$ git branch -dr [remote/branch]</span><br></pre></td></tr></table></figure>
<h2 id="六、标签"><a href="#六、标签" class="headerlink" title="六、标签"></a>六、标签</h2><blockquote>
<p>列出所有tag</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git tag</span><br></pre></td></tr></table></figure>
<blockquote>
<p>新建一个tag在当前commit</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git tag [tag]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>新建一个tag在指定commit</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git tag [tag] [commit]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>删除本地tag</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git tag -d [tag]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>删除远程tag</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin :refs/tags/[tagName]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>查看tag信息</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git show [tag]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>提交指定tag</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push [remote] [tag]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>提交所有tag</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push [remote] --tags</span><br></pre></td></tr></table></figure>
<blockquote>
<p>新建一个分支，指向某个tag</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -b [branch] [tag]</span><br></pre></td></tr></table></figure>
<h2 id="七、查看信息"><a href="#七、查看信息" class="headerlink" title="七、查看信息"></a>七、查看信息</h2><blockquote>
<p>显示有变更的文件</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git status</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示当前分支的版本历史</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git log</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示commit历史，以及每次commit发生变更的文件</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git log --stat</span><br></pre></td></tr></table></figure>
<blockquote>
<p>搜索提交历史，根据关键词</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git log -S [keyword]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示某个commit之后的所有变动，每个commit占据一行</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git log [tag] HEAD --pretty=format:%s</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示某个commit之后的所有变动，其”提交说明”必须符合搜索条件</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git log [tag] HEAD --grep feature</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示某个文件的版本历史，包括文件改名</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git log --follow [file]</span><br><span class="line">$ git whatchanged [file]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示指定文件相关的每一次diff</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git log -p [file]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示过去5次提交</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git log -5 --pretty --oneline</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示所有提交过的用户，按提交次数排序</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git shortlog -sn</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示指定文件是什么人在什么时间修改过</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git blame [file]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示暂存区和工作区的差异</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git diff</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示暂存区和上一个commit的差异</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git diff --cached [file]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示工作区与当前分支最新commit之间的差异</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git diff HEAD</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示两次提交之间的差异</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git diff [first-branch]...[second-branch]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示今天你写了多少行代码</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git diff --shortstat &quot;@&#123;0 day ago&#125;&quot;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示某次提交的元数据和内容变化</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git show [commit]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示某次提交发生变化的文件</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git show --name-only [commit]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示某次提交时，某个文件的内容</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git show [commit]:[filename]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示当前分支的最近几次提交</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git reflog</span><br></pre></td></tr></table></figure>
<h2 id="八、远程同步"><a href="#八、远程同步" class="headerlink" title="八、远程同步"></a>八、远程同步</h2><blockquote>
<p>下载远程仓库的所有变动</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git fetch [remote]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示所有远程仓库</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote -v</span><br></pre></td></tr></table></figure>
<blockquote>
<p>显示某个远程仓库的信息</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote show [remote]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>增加一个新的远程仓库，并命名</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote add [shortname] [url]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>取回远程仓库的变化，并与本地分支合并</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git pull [remote] [branch]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>上传本地指定分支到远程仓库</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push [remote] [branch]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>强行推送当前分支到远程仓库，即使有冲突</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push [remote] --force</span><br></pre></td></tr></table></figure>
<blockquote>
<p>推送所有分支到远程仓库</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push [remote] --all</span><br></pre></td></tr></table></figure>
<h2 id="九、撤销"><a href="#九、撤销" class="headerlink" title="九、撤销"></a>九、撤销</h2><blockquote>
<p>恢复暂存区的指定文件到工作区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout [file]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>恢复某个commit的指定文件到暂存区和工作区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout [commit] [file]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>恢复暂存区的所有文件到工作区</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout .</span><br></pre></td></tr></table></figure>
<blockquote>
<p>重置暂存区的指定文件，与上一次commit保持一致，但工作区不变</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git reset [file]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>重置暂存区与工作区，与上一次commit保持一致</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git reset --hard</span><br></pre></td></tr></table></figure>
<blockquote>
<p>重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git reset [commit]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git reset --hard [commit]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>重置当前HEAD为指定commit，但保持暂存区和工作区不变</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git reset --keep [commit]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>新建一个commit，用来撤销指定commit<br>后者的所有变化都将被前者抵消，并且应用到当前分支</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git revert [commit]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>暂时将未提交的变化移除，稍后再移入</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git stash</span><br><span class="line">$ git stash pop</span><br></pre></td></tr></table></figure>
<h2 id="十、其他"><a href="#十、其他" class="headerlink" title="十、其他"></a>十、其他</h2><blockquote>
<p>生成一个可供发布的压缩包</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git archive</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpeg"
               alt="Scott Du" />
          <p class="site-author-name" itemprop="name">Scott Du</p>
           
              <p class="site-description motion-element" itemprop="description">多读书 多看报 少吃零食 多睡觉</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Scott Du</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>
