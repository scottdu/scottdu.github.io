<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="tensorflow," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="搭建一个神经网络,总结搭建方法 基于Tensorflow的NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重（参数），得到模型。  张量(tensor)：多维数组（列表） 阶：张量的维数       维数 阶 名字 例子     0-D 0 标量 scalar s=123   1-D 1 向量 vector v=[1,2,3]   2-D 2 矩阵 matrix m=[[">
<meta name="keywords" content="tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow学习笔记">
<meta property="og:url" content="http://yoursite.com/2018/07/26/tensorflow-notebook/index.html">
<meta property="og:site_name" content="阿杜S考特">
<meta property="og:description" content="搭建一个神经网络,总结搭建方法 基于Tensorflow的NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重（参数），得到模型。  张量(tensor)：多维数组（列表） 阶：张量的维数       维数 阶 名字 例子     0-D 0 标量 scalar s=123   1-D 1 向量 vector v=[1,2,3]   2-D 2 矩阵 matrix m=[[">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-17-15318034642927.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-17-15318051293124.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-17-15318054268584.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-23-15323339395898.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-23-15323352101703.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-24-15324075893250.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-24-15324078150692.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-26-15325368607915.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-26-sgd.gif">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-26-sgd_bad.gif">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328790917732.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328792396453.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328793451699.jpg">
<meta property="og:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328793451699.jpg">
<meta property="og:updated_time" content="2018-07-30T17:29:00.848Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="tensorflow学习笔记">
<meta name="twitter:description" content="搭建一个神经网络,总结搭建方法 基于Tensorflow的NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重（参数），得到模型。  张量(tensor)：多维数组（列表） 阶：张量的维数       维数 阶 名字 例子     0-D 0 标量 scalar s=123   1-D 1 向量 vector v=[1,2,3]   2-D 2 矩阵 matrix m=[[">
<meta name="twitter:image" content="http://ovhbzkbox.bkt.clouddn.com/2018-07-17-15318034642927.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/07/26/tensorflow-notebook/"/>





  <title>tensorflow学习笔记 | 阿杜S考特</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">阿杜S考特</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">个人空间</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/26/tensorflow-notebook/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Scott Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://oueonj87a.bkt.clouddn.com/WechatIMG3.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="阿杜S考特">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">tensorflow学习笔记</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-26T00:45:14+08:00">
                2018-07-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="搭建一个神经网络-总结搭建方法"><a href="#搭建一个神经网络-总结搭建方法" class="headerlink" title="搭建一个神经网络,总结搭建方法"></a>搭建一个神经网络,总结搭建方法</h2><ul>
<li><p>基于Tensorflow的NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重（参数），得到模型。</p>
</li>
<li><p>张量(tensor)：多维数组（列表） 阶：张量的维数</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>维数</th>
<th>阶</th>
<th>名字</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>0-D</td>
<td>0</td>
<td>标量 scalar</td>
<td>s=123</td>
</tr>
<tr>
<td>1-D</td>
<td>1</td>
<td>向量 vector</td>
<td>v=[1,2,3]</td>
</tr>
<tr>
<td>2-D</td>
<td>2</td>
<td>矩阵 matrix</td>
<td>m=[[1,2,3],[4,5,6],[7,8,9]]</td>
</tr>
<tr>
<td>n-D</td>
<td>n</td>
<td>张量 tensor</td>
<td>t=[[[… （n个中括号）</td>
</tr>
</tbody>
</table>
</div>
<p>张量可以表示0阶到n阶数组(列表)</p>
<ul>
<li>数据类型：<code>tf.float32</code> <code>tf.int32</code> …</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line">result = a+b</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>显示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;add:0&quot;, shape=(2,), dtype=float.32)</span><br></pre></td></tr></table></figure>
<p>其中<code>add:</code>表示<code>节点名</code>，<code>0</code>表示<code>第0个输出</code>,<code>shape</code>表示<code>维度</code></p>
<p>注意vim</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.vimrc写入：</span><br><span class="line">set ts=4    # tab键转成4个空格</span><br><span class="line">set nu      # 显示行号</span><br></pre></td></tr></table></figure>
<ul>
<li>计算图(Graph)：搭建神经网络的计算过程，只搭建，不运算。</li>
</ul>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-17-15318034642927.jpg" width="200"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>]])</span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>],[<span class="number">4.0</span>]])</span><br><span class="line"></span><br><span class="line">y = tf.matmul(x,w)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>显示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"matmul:0"</span>,shape(<span class="number">1</span>,<span class="number">1</span>),dtype=float32)</span><br></pre></td></tr></table></figure>
<ul>
<li>会话(Session):执行计算图中的节点运算。</li>
</ul>
<p>计算<code>1.0*3.0+2.0*4.0=11.0</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>]])</span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>],[<span class="number">4.0</span>]])</span><br><span class="line"></span><br><span class="line">y = tf.matmul(x,w)</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"matmul:0"</span>,shape(<span class="number">1</span>,<span class="number">1</span>),dtype=float32)</span><br><span class="line">[[<span class="number">11.</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li>参数：即权重<code>w</code>，用变量表示，随机给初值。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(tf.random_normal([2,3], stddev=2, mean=0, seed=1))</span><br></pre></td></tr></table></figure>
<p>其中<code>tf.random_normal()</code>表示<code>正态分布</code>，<code>[2,3]</code>表示产生<code>2X3的矩阵</code>，<code>stddev</code>表示<code>标准差</code>，<code>mean</code>表示<code>均值</code>，<code>seed</code>表示<code>随机种子</code>;</p>
<p><code>tf.truncated_normal()</code>表示去掉过大偏离点的正态分布，<code>tf.random_uniform()</code>表示平均分布。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#tf.zeros全0数组</span><br><span class="line">tf.zeros([3,2],int32)  #生成[[0,0],[0,0],[0,0]]</span><br><span class="line"></span><br><span class="line">#tf.ones全1数组</span><br><span class="line">tf.ones([3,2],int32)  #生成[[1,1],[1,1],[1,1]]</span><br><span class="line"></span><br><span class="line">#tf.fill全定值数组</span><br><span class="line">tf.fll([3,2],6)    #生成[[6,6],[6,6],[6,6]]</span><br><span class="line"></span><br><span class="line">#tf.constant直接给值</span><br><span class="line">tf.constant([3,2,1]) # 生成[3,2,1]</span><br></pre></td></tr></table></figure>
<ul>
<li>神经网络实现过程：</li>
</ul>
<ol>
<li><p>准备数据集，提取特征，作为输入喂给神经网络<code>(Neural Network, NN)</code></p>
</li>
<li><p>搭建<code>NN</code>结构，从输入到输出(先搭建计算图，再用会话执行)</p>
<blockquote>
<p>NN前向传播算法 =&gt; 计算输出</p>
</blockquote>
</li>
<li><p>大量特征数据喂给<code>NN</code>，迭代优化<code>NN</code>参数</p>
<blockquote>
<p>NN反向传播算法 =&gt; 优化参数训练模型</p>
</blockquote>
</li>
<li><p>使用训练好的模型预测和分类</p>
</li>
</ol>
<ul>
<li>前向传播 =&gt; 搭建模型，实现推理（以全连接网络为例子）</li>
</ul>
<p>eg. 生产一批零件将体积x1和重量x2为特征输入NN，通过NN后输出一个数值。</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-17-15318051293124.jpg" alt=""></p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-17-15318054268584.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.matmul(X, W1)</span><br><span class="line">y = tf.matmul(a, W2)</span><br></pre></td></tr></table></figure>
<ul>
<li>变量初始化、计算图节点，运算都要用会话（with结构）实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run()</span><br></pre></td></tr></table></figure>
<ul>
<li>变量初始化：在<code>sess.run</code>函数中用<code>tf.global_variables_initializer()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line">sess.run(init_op)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算图节点运算：在<code>sess.run</code>函数中写入待运算的节点</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(y)</span><br></pre></td></tr></table></figure>
<ul>
<li>用<code>tf.placeholder</code>占位，在<code>sess.run</code>函数中用<code>feed_dict</code>喂数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 喂一组数据</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">sess.run(y, feed_dict=&#123;x:[[<span class="number">0.5</span>, <span class="number">0.6</span>]]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 喂多组数据</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>,<span class="number">2</span>))</span><br><span class="line">sess.run(y, feed_dict=&#123;x:[[<span class="number">0.1</span>,<span class="number">0.2</span>],[<span class="number">0.2</span>,<span class="number">0.3</span>],[<span class="number">0.3</span>,<span class="number">0.4</span>],[<span class="number">0.4</span>,<span class="number">0.5</span>]]&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># coding:utf-8</span><br><span class="line"># 两层简单神经网络(全连接)</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># 定义输入和参数</span><br><span class="line">x = tf.constant([[0.7,0.5]])</span><br><span class="line">W1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))</span><br><span class="line">W2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))</span><br><span class="line"></span><br><span class="line"># 定义前向传播过程</span><br><span class="line">a = tf.matmul(x, W1)</span><br><span class="line">y = tf.matmul(a, W2)</span><br><span class="line"></span><br><span class="line"># 用会话计算结果</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(&quot;y : \n&quot;, sess.run(y))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"># y : </span><br><span class="line"># [[ 3.0904665]]</span><br></pre></td></tr></table></figure>
<ul>
<li>参数：即权重w，用变量表示，随机给初值。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(tf.random_normal([2,3], stddev=2, mean=0, seed=1))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>其中tf.random_normal()表示正态分布，<br>[2,3]表示产生2X3的矩阵，<br>stddev表示标准差，<br>mean表示均值，<br>seed表示随机种子;<br>tf.truncated_normal()表示去掉过大偏离点的正态分布<br>tf.random_uniform()表示平均分布。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#tf.zeros全0数组</span><br><span class="line">tf.zeros([3,2],int32)  #生成[[0,0],[0,0],[0,0]]</span><br><span class="line"></span><br><span class="line">#tf.ones全1数组</span><br><span class="line">tf.ones([3,2],int32)  #生成[[1,1],[1,1],[1,1]]</span><br><span class="line"></span><br><span class="line">#tf.fill全定值数组</span><br><span class="line">tf.fll([3,2],6)    #生成[[6,6],[6,6],[6,6]]</span><br><span class="line"></span><br><span class="line">#tf.constant直接给值</span><br><span class="line">tf.constant([3,2,1]) # 生成[3,2,1]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 两层简单神经网络(全连接)</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line"><span class="comment"># 用placeholder定义输入(sess.run喂多维数据)</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">W1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">W2 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, W1)</span><br><span class="line">y = tf.matmul(a, W2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(<span class="string">"y: \n"</span>, sess.run(y, feed_dict=&#123;x:[[<span class="number">0.7</span>,<span class="number">0.5</span>],[<span class="number">0.2</span>,<span class="number">0.3</span>],[<span class="number">0.3</span>,<span class="number">0.4</span>],[<span class="number">0.4</span>,<span class="number">0.5</span>]]&#125;))</span><br><span class="line">    print(<span class="string">"W1: \n"</span>, sess.run(W1))</span><br><span class="line">    print(<span class="string">"W2: \n"</span>, sess.run(W2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">y: </span><br><span class="line"> [[ <span class="number">3.0904665</span> ]</span><br><span class="line"> [ <span class="number">1.2236414</span> ]</span><br><span class="line"> [ <span class="number">1.72707319</span>]</span><br><span class="line"> [ <span class="number">2.23050475</span>]]</span><br><span class="line">W1: </span><br><span class="line"> [[<span class="number">-0.81131822</span>  <span class="number">1.48459876</span>  <span class="number">0.06532937</span>]</span><br><span class="line"> [<span class="number">-2.4427042</span>   <span class="number">0.0992484</span>   <span class="number">0.59122431</span>]]</span><br><span class="line">W2: </span><br><span class="line"> [[<span class="number">-0.81131822</span>]</span><br><span class="line"> [ <span class="number">1.48459876</span>]</span><br><span class="line"> [ <span class="number">0.06532937</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li><p>反向传播 =&gt; 训练模型参数，在所有参数上用梯度下降，使<code>NN</code>模型在训练数据上的损失函数最小。</p>
</li>
<li><p>损失函数<code>loss</code>：预测值<code>y</code>与已知答案<code>y_</code>的差距</p>
</li>
<li><p>均方误差<code>MSE</code></p>
</li>
</ul>
<script type="math/tex; mode=display">MSE(y\_,y) = \frac{\sum_{i=1}^n(y-y\_)^2}{n}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.square(y_-y))</span><br></pre></td></tr></table></figure>
<ul>
<li>反向传播训练方法：以减小<code>loss</code>值为优化目标</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span><br><span class="line"></span><br><span class="line">train_step = tf.train.MomentumOptimizer(learning_rate).minimize(loss)</span><br><span class="line"></span><br><span class="line">train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)</span><br></pre></td></tr></table></figure>
<ul>
<li>学习率<code>learning_rate</code>：决定参数每次更新的幅度</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">#encoding:utf-8</span><br><span class="line">#0 导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">seed = 23455</span><br><span class="line"></span><br><span class="line"># 基于seed产生随机数</span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"></span><br><span class="line"># 随机数返回32行2列的矩阵 表示32组 体积和重量 作为输入数据集</span><br><span class="line">X = rng.rand(32,2)</span><br><span class="line"></span><br><span class="line"># 从X这个32行2列的矩阵中 取出一行 判断如果和小于1 给Y赋值1 如果不小于1 给Y赋值0</span><br><span class="line"># 作为输入数据集的标签(正确答案)</span><br><span class="line">Y = [[int(x0 + x1 &lt; 1)] for (x0,x1) in X]</span><br><span class="line">print(&quot;X:\n&quot;,X)</span><br><span class="line">print(&quot;Y:\n&quot;,Y)</span><br><span class="line"></span><br><span class="line">#1 定义神经网络的输入、参数和输出，定义前向传播过程</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))</span><br><span class="line"></span><br><span class="line">a = tf.matmul(x,w1)</span><br><span class="line">y = tf.matmul(a,w2)</span><br><span class="line"></span><br><span class="line">#2 定义损失函数及方向传播方法</span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)</span><br><span class="line">#train_step = tf.train.MomentumOptimizer(0.01,0.9).minimize(loss)</span><br><span class="line">#train_step = tf.train.AdamOptimizer(0.01).minimize(loss)</span><br><span class="line"></span><br><span class="line">#3 生成会话，训练STEP轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  # 输出目前(未经训练)的参数取值</span><br><span class="line">  print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">  print(&quot;w2:\n&quot;, sess.run(w2))</span><br><span class="line">  </span><br><span class="line">  # 训练模型</span><br><span class="line">  STEPS = 3000</span><br><span class="line">  for i in range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % 32</span><br><span class="line">    end = start + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x:X[start:end], y_: Y[start:end]&#125;)</span><br><span class="line">    if i % 500 == 0:</span><br><span class="line">      total_loss = sess.run(loss, feed_dict=&#123;x:X,y_:Y&#125;)</span><br><span class="line">      print(&quot;After %d training step(s), loss on all data is %g&quot; % (i, total_loss))</span><br><span class="line">      </span><br><span class="line">    # 输出训练后的参数取值</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;w2:\n&quot;, sess.run(w2))</span><br></pre></td></tr></table></figure>
<ul>
<li>神经元模型：用数学公式表示为: $f(\sum_ix_iw_i+b)$，$f$为激活函数。神经网络是以神经元为基本单元构成的。</li>
</ul>
<ul>
<li>激活函数：引入非线性激活因素，提高模型的表达力。</li>
</ul>
<blockquote>
<p>(1) 常用的激活函数<code>relu</code>：在Tensorflow中，用<code>tf.nn.relu()</code>表示</p>
</blockquote>
<script type="math/tex; mode=display">f(x)=max(x,0)=\begin{cases}0 & x <= 0 \\ x & x >= 0\end{cases}</script><p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-23-15323339395898.jpg" width="200"></p>
<blockquote>
<p>(2) 激活函数<code>sigmoid</code>：在Tensorflow中，用<code>tf.nn.sigmoid()</code>表示</p>
</blockquote>
<script type="math/tex; mode=display">f(x)=\frac{1}{1+e^{-x}}</script><p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-23-15323352101703.jpg" width="200"></p>
<blockquote>
<p>(3) 激活函数<code>tanh</code>：在tensorflow中，用<code>tf.nn.tanh()</code>表示</p>
</blockquote>
<script type="math/tex; mode=display">f(x)=\frac{1-e^{-2x}}{1+e^{-2x}}</script><p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-24-15324075893250.jpg" width="200"></p>
<ul>
<li><p>神经网络的复杂度：可用神经网络的层数和神经网络中待优化的参数个数表示</p>
</li>
<li><p>神经网络的层数：一般不计入输入层，层数=n个隐藏层+1个输出层</p>
</li>
<li><p>神经网络待优化的参数：神经网络中所有参数<code>w</code>的个数+所有参数<code>b</code>的个数</p>
</li>
</ul>
<blockquote>
<p>例如:</p>
</blockquote>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-24-15324078150692.jpg" width="200"></p>
<blockquote>
<p>该神经网络中，包含1个输入层、1个隐藏层和1个输出层，该神经网络的层数为2层。<br>在该神经网络中，参数的个数是所有参数<code>w</code>的个数加上所有参数<code>b</code>的总数，第一层参数用3行4列的二阶张量表示（即12个线上的权重<code>w</code>）再加4个偏置<code>b</code>；第二层参数是4行2列的二阶张量（即8个线上的权重<code>w</code>）在加上2个偏置<code>b</code>。总参数=3X4+4+4X2+2=26</p>
</blockquote>
<ul>
<li><p>损失函数<code>loss</code>：用来表示预测值<code>y</code>与已知答案<code>y_</code>的差距。在训练神经网络时，通过不断改变神经网络中所有参数，使损失函数不断减小，从而训练出更高准确率的神经网络模型。</p>
</li>
<li><p>常用的损失函数有均方误差、自定义和交叉熵等。</p>
</li>
<li><p>均方误差<code>mse</code>：<code>n</code>个样本的预测值<code>y</code>与已知答案<code>y_</code>的平方和，再求均值。</p>
</li>
</ul>
<script type="math/tex; mode=display">MSE(y\_,y) = \frac{\sum_{i=1}^n(y - y\_)^2}{n}</script><blockquote>
<p>在Tensorflow中用</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_mse = tf.reduce_mean(tf.square(y_-y))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>例如：预测酸奶日销量<code>y</code>,<code>x1</code>,<code>x2</code>是影响销量的两个因素。<br>应提前采集的数据有：一段时间内，每日的<code>x1</code>因素、<code>x2</code>因素和销量<code>y_</code>。采集的数据尽量多。本例中用销量预测产量，最优的产量应该等于销量。由于目前没有数据集，所以拟造了一套数据集。利用Tensorflow中函数随机生成<code>x1</code>,<code>x2</code>，制造标准答案<code>y_=x1+x2</code>，为了更真实，求和后还加了正负0.05的随机噪声。我们把这套自制的数据集喂入神经网络，构建一个一层的神经网络，拟合预测酸奶日销量的函数。</p>
</blockquote>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#预测多或少的影响一样</span></span><br><span class="line"><span class="comment">#0 导入模块，生成数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">Y = [[x1+x2+(rdm.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment">#1 定义神经网络的输入、参数和输出，定义前向传播的过程。</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2 定义损失函数及反向传播方法。</span></span><br><span class="line"><span class="comment"># 定义损失函数为MSE，反向传播方法为梯度下降。</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y_-y))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss_mes)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3 生成会话，训练STEPS轮</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  STEPS = <span class="number">20000</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">    end = (i*BATCH_SIZE) % <span class="number">32</span> + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y_[start:end]&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">      print(<span class="string">"After %d training steps, w1 is: "</span> % (i))</span><br><span class="line">      print(sess.run(w1),<span class="string">"\n"</span>)</span><br><span class="line">print(<span class="string">"Final w1 is: \n"</span>, sess.run(w1))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">After 19000 training steps, w1 is:</span><br><span class="line">[[0.974931  ]</span><br><span class="line"> [1.02062762]]</span><br><span class="line"> </span><br><span class="line">After 19500 training steps, w1 is:</span><br><span class="line">[[0.97770262]</span><br><span class="line"> [1.01819491]]</span><br><span class="line"> </span><br><span class="line">Final w1 is：</span><br><span class="line">[[0.98019385]</span><br><span class="line"> [1.0159872 ]]</span><br></pre></td></tr></table></figure>
<p>由上述代码可知，本例中神经网络预测模型为 <script type="math/tex">y=w_1x_1+w_2x_2</script> ，损失函数采用均方误差。通过使损失函数值<code>loss</code>不断降低，神经网络模型得到最终参数<script type="math/tex">x_{1}=0.98, w_{2}=1.02</script>，销量预测结果为<script type="math/tex">y=0.98*x_1+1.02*x_2</script>。由于在生成数据集时，标准答案为<script type="math/tex">y=x_1+x_2</script>，因此，销量预测结果和标准答案已非常接近，说明该神经网络预测正确。</p>
<ul>
<li>自定义损失函数：根据问题的实际情况，定制合理的损失函数。</li>
</ul>
<p>例如：对于预测酸奶日销量问题，如果预测销量大于实际销量会损失成本；如果预测销量小于实际销量则会损失利润。在实际生活中，往往制造一盒酸奶的成本和销售一盒酸奶的利润是不等价的。因此，需要使用符合该问题的自定义损失函数。</p>
<p>自定义损失函数为：<script type="math/tex">loss=\sum_{n}f(y\_,y)</script><br>其中，损失定义成分段函数：</p>
<script type="math/tex; mode=display">f(y\_，y)=\begin{cases}PROFIT*(y\_-y) & y < y\_\\ COST*(y-y\_) & y \geq y\_\end{cases}</script><p>损失函数表示，若预测结果<code>y</code>小于标准答案<code>y_</code>，损失函数为利润乘以预测结果<code>y</code>与标准答案<code>y_</code>之差；若预测结果<code>y</code>大于标准答案<code>y_</code>，损失函数为成本乘以预测结果<code>y</code>与标准答案<code>y_</code>之差。</p>
<p>用Tensorflow函数表示为:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y,y_), COST(y,y_), PROFIT(y_,y)))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(1)若酸奶成本为1元，酸奶销售利润为9元，则制造成本小于酸奶利润，因此希望预测的结果<code>y</code>多一些。采用上述的自定义损失函数，训练神经网络。</p>
</blockquote>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#酸奶成本1元，酸奶利润9元</span><br><span class="line">#预测少了损失，故不要预测少，故生成的模型会多预测一些</span><br><span class="line">#0 导入模块，生成数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">SEED = 23455</span><br><span class="line">COST = 1</span><br><span class="line">PROFIT = 9</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(32,2)</span><br><span class="line">Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in X]</span><br><span class="line"></span><br><span class="line">#1 定义神经网络的输入、参数和输出，定义前向传播的过程。</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))</span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line">#2 定义损失函数及反向传播方法。</span><br><span class="line"># 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。</span><br><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y,y_)*COST,(y_-y)*PROFIT))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)</span><br><span class="line"></span><br><span class="line">#3 生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  STEPS = 20000</span><br><span class="line">  for i in range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % 32</span><br><span class="line">    end = (i*BATCH_SIZE) % 32 + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y_[start:end]&#125;)</span><br><span class="line">    if i % 500 == 0:</span><br><span class="line">      print(&quot;After %d training steps, w1 is: &quot; % (i))</span><br><span class="line">      print(sess.run(w1),&quot;\n&quot;)</span><br><span class="line">print(&quot;Final w1 is: \n&quot;, sess.run(w1))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">After 2000 training steps, w1 is:</span><br><span class="line">[[1.01793861]</span><br><span class="line"> [1.04128993]]</span><br><span class="line"> </span><br><span class="line">After 2500 training steps, w1 is:</span><br><span class="line">[[1.02059376]</span><br><span class="line"> [1.03906775]]</span><br><span class="line"> </span><br><span class="line">Final w1 is:</span><br><span class="line">[[1.02965927]</span><br><span class="line"> [1.0484432 ]]</span><br></pre></td></tr></table></figure>
<p>由代码执行结果可知，神经网络最终参数为 $w_1=1.03$，$ w_2=1.05$，销量预测结果为 $y =1.03x_1 + 1.05x_2$。由此可见，采用自定义损失函数预测的结果大于采用均方误差预测的结果，更符合实际需求。</p>
<blockquote>
<p>(2)若酸奶成本为9元，酸奶销售利润为1元，则制造成本大于酸奶利润，因此希望预测结果$y$小一 些。采用上述的自定义损失函数，训练神经网络模型。</p>
</blockquote>
<p>代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#酸奶成本1元，酸奶利润9元</span><br><span class="line">#预测少了损失，故不要预测少，故生成的模型会多预测一些</span><br><span class="line">#0 导入模块，生成数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">SEED = 23455</span><br><span class="line">COST = 9</span><br><span class="line">PROFIT = 1</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(32,2)</span><br><span class="line">Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in X]</span><br><span class="line"></span><br><span class="line">#1 定义神经网络的输入、参数和输出，定义前向传播的过程。</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))</span><br><span class="line">y = tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line">#2 定义损失函数及反向传播方法。</span><br><span class="line"># 定义损失函数使得预测多了的损失大，于是模型应该偏向少的方向预测。</span><br><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y,y_)*COST,(y_-y)*PROFIT))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)</span><br><span class="line"></span><br><span class="line">#3 生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  STEPS = 20000</span><br><span class="line">  for i in range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % 32</span><br><span class="line">    end = (i*BATCH_SIZE) % 32 + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y_[start:end]&#125;)</span><br><span class="line">    if i % 500 == 0:</span><br><span class="line">      print(&quot;After %d training steps, w1 is: &quot; % (i))</span><br><span class="line">      print(sess.run(w1),&quot;\n&quot;)</span><br><span class="line">print(&quot;Final w1 is: \n&quot;, sess.run(w1))</span><br></pre></td></tr></table></figure>
<p>运行结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">After 2000 training steps, w1 is:</span><br><span class="line">[[0.96024752]</span><br><span class="line"> [0.97420841]]</span><br><span class="line"></span><br><span class="line">After 2500 training steps, w1 is:</span><br><span class="line">[[0.96100295]</span><br><span class="line"> [0.96993417]]</span><br><span class="line"></span><br><span class="line">Final w1 is</span><br><span class="line">[[0.96004069]</span><br><span class="line"> [0.97334176]]</span><br></pre></td></tr></table></figure>
<p>由执行结果可知，神经网络最终参数为$w1=0.96$，$w2=0.97$，销量预测结果为$y=0.96x_1 + 0.97x_2$。 因此，采用自定义损失函数预测的结果小于采用均方误差预测的结果，更符合实际需求。</p>
<ul>
<li>交叉熵<code>Cross Entropy</code>:表示两个概率分布之间的距离。交叉熵越大，两个概率分布距离越远，两个概率分布越相异;交叉熵越小，两个概率分布距离越近，两个概率分布越相似。 交叉熵计算公式:<script type="math/tex">H(y\_,y)=-\sum{y\_\log{y}}</script></li>
</ul>
<p>用 Tensorflow 函数表示为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ce = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-2,1.0)))</span><br></pre></td></tr></table></figure>
<p>例如:两个神经网络模型解决二分类问题中，已知标准答案为<code>y_=(1,0)</code>，第一个神经网络模型预测结果为<code>y1=(0.6,0.4)</code>，第二个神经网络模型预测结果为<code>y2=(0.8,0.2)</code>，判断哪个神经网络模型预测的结果更接近标准答案？</p>
<p>根据交叉熵的计算公式得:</p>
<p><code>H1((1,0),(0.6,0.4)) = -(1*log0.6 + 0*log0.4) ≈ -(-0.222 + 0) = 0.222</code></p>
<p><code>H2((1,0),(0.8,0.2)) = -(1*log0.8 + 0*log0.2) ≈ -(-0.097 + 0) = 0.097</code></p>
<p>由于<code>0.222 &gt; 0.097</code>，所以预测结果<code>y2</code>与标准答案<code>y_</code>更接近，<code>y2</code>预测更准确。</p>
<ul>
<li>Softmax函数：将<code>n</code>分类的<code>n</code>个输出<code>(y1,y2,...,yn)</code>变为满足以下概率分布要求的函数。</li>
</ul>
<p>$\forall{x}$ 有 $P(X=x)\in[0,1]$ 且 $\sum{P_x(X=x)}=1$</p>
<p>softmax函数表示为<script type="math/tex">softmax(y_i)=\frac{e^{y_{i}}}{\sum_{j=1}^{n}e^{y_{i}}}</script></p>
<p>softmax函数应用：在<code>n</code>分类中，模型会有<code>n</code>个输出，即y1,y2,…,yn，其中yi表示第i中情况的可能性大小。将<code>n</code>个输出经过softmax函数，可得到符合概率分布的分类结果。</p>
<ul>
<li>在Tensorflow中，一般让模型经过softmax函数，以获得输出分类的概率分布，再与标准答案对比，求出交叉熵，得到损失函数，用如下函数实现：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))</span><br><span class="line">cem = tf.reduce_mean(ce)</span><br></pre></td></tr></table></figure>
<ul>
<li>学习率<code>learning_rate</code>:表示了每次参数更新的幅度大小。学习率过大，会导致待优化的参数在最小值附近波动，不收敛；学习率过小，会导致待优化的参数收敛缓慢。在训练过程中，参数的更新向着损失函数梯度下降的方向。参数的更新公式为<script type="math/tex; mode=display">w_{n+1}=w_{n}-learning\_rate\Delta</script></li>
</ul>
<p>假设损失函数为$loss=(w+1)^2$。梯度是损失函数<code>loss</code>的导数$\Delta=2w+2$。如参数初值为5，学习率为0.2，则参数和损失函数更新如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1次  参数w:5       5-0.2*(2*5+2)=2.6</span><br><span class="line">2次  参数w:2.6     2.6-0.2*(2*2.6+2)=1.16</span><br><span class="line">3次  参数w:1.16    1.16-0.2*(2*1.16+2)=0.296</span><br><span class="line">4次  参数w:0.296</span><br></pre></td></tr></table></figure>
<p>损失函数$loss=(w+1)^2$的图像为：</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-26-15325368607915.jpg" width="200"></p>
<p>由图可知，损失函数loss的最小值会在(-1,0)处得到，此时损失函数的导数为0,得到最终参数w=-1。代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#encoding:utf-8</span><br><span class="line">#设损失函数loss=(w+1)^2，令w初值是常数5。反向传播就是求最优y，即求最小loss对应的w值</span><br><span class="line">import tensorflow as tf</span><br><span class="line">#定义待优化参数w初值赋5</span><br><span class="line">w = tf.Variable(tf.constant(5,dtype=tf.float32))</span><br><span class="line"></span><br><span class="line">#定义损失函数loss</span><br><span class="line">loss = tf.square(w+1)</span><br><span class="line">#定义反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#生成会话，训练40轮</span><br><span class="line">with tf.Session() as tf:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  for i in rang(40):</span><br><span class="line">    sess.run(train_step)</span><br><span class="line">    w_val = sess.run(w)</span><br><span class="line">    loss_val = sess.run(loss)</span><br><span class="line">    print(&quot;After %s steps: w is %f, loss is %f.&quot; % (i, w_val, loss_val))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">After 30 steps: w is -0.999999, loss is 0.000000 </span><br><span class="line">After 31 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 32 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 33 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 34 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 35 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 36 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 37 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 38 steps: w is -1.000000, loss is 0.000000</span><br><span class="line">After 39 steps: w is -1.000000, loss is 0.000000</span><br></pre></td></tr></table></figure>
<p>由结果可知，随着损失函数值的减小，<code>w</code>无限趋近于-1，模型计算推测出最优参数<code>w=-1</code>。</p>
<ul>
<li>学习率的设置，学习率过大，会导致待优化的参数在最小值附近波动，不收敛；学习率过小，会导致优化参数收敛缓慢</li>
</ul>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-26-sgd.gif" alt="sgd"><br><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-26-sgd_bad.gif" alt="sgd_bad"></p>
<p>例如：(1)对于上例的损失函数$loss=(w+1)^2$。则将上述代码中学习率修改为1，其余内容不变。实验结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">After 11 steps: w is 5.000000, loss is 36.000000 </span><br><span class="line">After 12 steps: w is -7.000000, loss is 36.000000</span><br><span class="line">After 13 steps: w is 5.000000, loss is 36.000000</span><br><span class="line">After 14 steps: w is -7.000000, loss is 36.000000</span><br><span class="line">After 15 steps: w is 5.000000, loss is 36.000000</span><br><span class="line">After 16 steps: w is -7.000000, loss is 36.000000</span><br></pre></td></tr></table></figure>
<p>运行结果可知，损失函数<code>loss</code>值并没有收敛，而是在5和-7之间波动。</p>
<p>（2）对于上例的损失函数$loss=(w+1)^2$。则将上述代码中学习率修改为0.0001，其余内容不变。实验结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">After 31 steps: w is 4.961716, loss is 35.542053 </span><br><span class="line">After 32 steps: w is 4.960523, loss is 35.527836</span><br><span class="line">After 33 steps: w is 4.959311, loss is 35.513626</span><br><span class="line">After 34 steps: w is 4.958139, loss is 35.499420</span><br><span class="line">After 35 steps: w is 4.956947, loss is 35.485222</span><br><span class="line">After 36 steps: w is 4.955756, loss is 35.471027</span><br><span class="line">After 37 steps: w is 4.954565, loss is 35.456841</span><br><span class="line">After 38 steps: w is 4.953373, loss is 35.442654</span><br><span class="line">After 39 steps: w is 4.952183, loss is 35.428478</span><br></pre></td></tr></table></figure>
<p>由运行结果可知，损失函数<code>loss</code>值缓慢下降，<code>w</code>值也在小幅度变化，收敛缓慢。</p>
<ul>
<li>指数衰减学习率: 学习率随着训练轮数变化而动态更新</li>
</ul>
<script type="math/tex; mode=display">Learning\_rate=LEARNING\_RATE\_BASE*LEARNING\_RATE\_DECY*\frac{global\_step}{LEARNING\_RATE\_BATCH\_SIZE}</script><p>用Tensorflow的函数表示为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Varibale(0, trainable=False)</span><br><span class="line">learning_rate = tf.train.exponential_decy(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP,LEARNING_RATE_DECAY,staircase=True/False)</span><br></pre></td></tr></table></figure>
<p>其中，<code>LEARNING_RATE_BASE</code>为学习率初始值，<code>LEARNING_RATE_DECAY</code>为学习率衰减率,<code>global_step</code>记录了当前训练轮数，为不可训练型参数。学习率<code>learning_rate</code>更新频率为输入数据集总样本数除以每次喂入样本数。若<code>staircase</code>设置为<code>True</code>时，表示 <code>global_step/learning_rate_step</code>取整数，学习率阶梯型衰减;若<code>staircase</code>设置为<code>false</code>时，学习率会是一条平滑下降的曲线。</p>
<p>例如:在本例中，模型训练过程不设定固定的学习率，使用指数衰减学习率进行训练。其中，学习率初值设置为 0.1，学习率衰减率设置为0.99，BATCH_SIZE设置为1。</p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#encoding:utf-8</span><br><span class="line">#设损失函数loss=(w+2)^2，令w初值是常数10。反向传播就是求最优w，即求最小loss对应的w值</span><br><span class="line">#使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得更有收敛度。</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = 0.1   #初始学习率</span><br><span class="line">LEARNING_RATE_DECAY = 0.99 #学习衰减率</span><br><span class="line">LEARNING_RATE_STEP = 1     #喂入多少轮BATCH_SIZE后，更新一次学习率，一般设为：总样本数/BATCH_SIZE</span><br><span class="line"></span><br><span class="line">#运行了几轮BATCH_SIZE的计数器，初值给0，设为不被训练</span><br><span class="line">global_step = tf.Variable(0, trainable=False)</span><br><span class="line">#定义指数下降学习率</span><br><span class="line">learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP, LEARNING_RATE_DECAY, staircase=True)</span><br><span class="line">#定义待优化参数，初值给10</span><br><span class="line">w = tf.Variable(tf.constant(5, dtype=tf.float32))</span><br><span class="line">#定义损失函数loss</span><br><span class="line">loss = tf.square(w+1)</span><br><span class="line">#定义反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line">#生成会话训练40轮</span><br><span class="line">with tf.Session() as tf:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  for i in range(40):</span><br><span class="line">    sess.run(train_step)</span><br><span class="line">    learning_rate_val = sess.run(learning_rate)</span><br><span class="line">    global_step_val = sess.run(global_step)</span><br><span class="line">    w_val = sess.run(w)</span><br><span class="line">    loss_val = sess.run(loss)</span><br><span class="line">    print(&quot;After %s steps: global_step is %f, w is %f, learning rate is %f, loss is %f&quot; % (i, global_step_val, w_val, learning_rate_val, loss_val))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">After 35 steps: global_step is 36.000000, w is -0.992297, learning_rate is 0.069641, loss is 0.000059</span><br><span class="line">After 36 steps: global_step is 37.000000, w is -0.993369, learning_rate is 0.068945, loss is 0.000044</span><br><span class="line">After 37 steps: global_step is 38.000000, w is -0.994284, learning_rate is 0.068255, loss is 0.000033</span><br><span class="line">After 38 steps: global_step is 39.000000, w is -0.995064, learning_rate is 0.067573, loss is 0.000024</span><br><span class="line">After 39 steps: global_step is 40.000000, w is -0.995731, learning_rate is 0.066897, loss is 0.000018</span><br></pre></td></tr></table></figure>
<p>由结果可以看出，随着训练轮数增加学习率在不断减小。</p>
<ul>
<li>滑动平均:记录了一段时间内模型中所有参数<code>w</code>和<code>b</code>各自的平均值。利用滑动平均值可以增强模型的泛化能力。</li>
<li>滑动平均值(影子)计算公式:<code>影子=衰减率*影子+(1-衰减率)*参数</code>;其中，</li>
</ul>
<script type="math/tex; mode=display">衰减率=min\{MOVING_{AVERAGE_{DECAY'}}\frac{1+轮数}{10+轮数}\}</script><p>影子初值=参数初值</p>
<ul>
<li>用Tensorflow函数表示为：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, gloabl_step)</span><br></pre></td></tr></table></figure>
<p>其中，<code>MOVING_AVERAGE_DECAY</code>表示滑动平均衰减率，一般会赋接近1的值，<code>global_step</code>表示当前训练了多少轮。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br></pre></td></tr></table></figure>
<p>其中，<code>ema.apply()</code>函数实现对括号内参数求滑动平均，<code>tf.trainable_variables()</code>函数实现把所有 待训练参数汇总为列表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.control_dependencies([train_step, ema_op]):  train_op = tf.no_op(name=&apos;train&apos;)</span><br></pre></td></tr></table></figure>
<p>其中，该函数实现将滑动平均和训练过程同步运行。</p>
<p>查看模型中参数的平均值，可以用 <code>ema.average()</code>函数。</p>
<p>例如:<br>在神经网络模型中，将<code>MOVING_AVERAGE_DECAY</code>设置为0.99，参数<code>w1</code>设置为0，<code>w1</code>的滑动平均值设 置为0。</p>
<p>(1)开始时，轮数<code>global_step</code>设置为0，参数<code>w1</code>更新为1，则<code>w1</code>的滑动平均值为:<br><code>w1</code>，<code>滑动平均值=min(0.99,1/10)*0+(1– min(0.99,1/10)*1 = 0.9</code></p>
<p>(2)当轮数<code>global_step</code>设置为100时，参数<code>w1</code>更新为10，以下代码<code>global_step</code>保持为100，每次执行滑动平均操作影子值更新，则滑动平均值变为:<code>w1滑动平均值=min(0.99,101/110)*0.9+(1– min(0.99,101/110)*10 = 0.826+0.818=1.644</code></p>
<p>(3)再次运行，参数<code>w1</code>更新为1.644，则滑动平均值变为:<code>w1滑动平均值=min(0.99,101/110)*1.644+(1– min(0.99,101/110)*10 = 2.328</code></p>
<p>(4)再次运行，参数<code>w1</code>更新为2.328，则滑动平均值:<code>w1滑动平均值=2.956</code></p>
<p>代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">#encoding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#1. 定义变量及滑动平均类</span><br><span class="line"># 定义一个32位浮点变量，初始值为0.0 这个代码就是不断更新w1参数，优化w1参数，滑动平均做了个w1的影子</span><br><span class="line">w1 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">#定义num_updates(NN的迭代轮数)，初始值为0，不可被优化（训练），这个参数不训练</span><br><span class="line">global_step = tf.Variable(0, trainable=False)</span><br><span class="line"></span><br><span class="line">#实例化滑动平均类，给删减率为0.99，当前轮global_step</span><br><span class="line">MOVING_AVERAGE_DECAY = 0.99</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line"></span><br><span class="line">#ema.apply后的括号里是更新列表，每次运行sess.run(ema_op)时，对更新列表中的元素求滑动平均值</span><br><span class="line">#在实际应用中会使用tf.trainable_variables()自动将所有待训练的参数汇总为列表</span><br><span class="line">#ema_op = ema.apply([w1])</span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line">#2. 查看不同迭代中变量取值的变化</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  #初始化</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  #用ema.average(w1)获取w1滑动平均值（要运行多个节点，作为列表中的元素列出，写在sess.run中）</span><br><span class="line">  #打印出当前参数w1和w1滑动平均值</span><br><span class="line">  print(sess.run([w1, ema.average(w1)]))</span><br><span class="line">  </span><br><span class="line">  #参数w1的值赋为1</span><br><span class="line">  sess.run(tf.assign(w1,1))</span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line">  </span><br><span class="line">  #更新step的w1的值，模拟出100轮迭代后，参数w1变为10</span><br><span class="line">  sess.run(tf.assign(global_step, 100))</span><br><span class="line">  sess.run(tf.assign(w1, 10))</span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1, ema.average(w1)]))</span><br><span class="line">  </span><br><span class="line">  #每次sess.run会更新一次w1的滑动平均值</span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line">  </span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">  sess.run(ema_op)</span><br><span class="line">  print(sess.run([w1,ema.average(w1)]))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[0.0, 0.0]</span><br><span class="line">[1.0, 0.89999998]</span><br><span class="line">[10.0, 1.6445453]</span><br><span class="line">[10.0, 2.3281732]</span><br><span class="line">[10.0, 2.955868]</span><br><span class="line">[10.0, 3.5322061]</span><br><span class="line">[10.0, 4.061389]</span><br><span class="line">[10.0, 4.5472751]</span><br><span class="line">[10.0, 4.9934072]</span><br></pre></td></tr></table></figure>
<p>从运行结果可知，最初参数<code>w1</code>和滑动平均值都是0;参数<code>w1</code>设定为1后，滑动平均值变为0.9;当迭代轮数更新为100轮时，参数<code>w1</code>更新为10后，滑动平均值变为1.644。随后每执行一次，参数<code>w1</code>的滑动平均值都向参数<code>w1</code>靠近。可见，滑动平均追随参数的变化而变化。</p>
<ul>
<li>过拟合：神经网络模型在训练数据集上的准确率较高，在新的数据进行预测或分类时准确率较低，说明模型的泛化能力差。</li>
<li>正则化：在损失函数中给每个参数<code>w</code>加上权重，引入模型复杂度指标，从而抑制模型噪声，减小过拟合。</li>
</ul>
<p>使用正则化后，损失函数<code>loss</code>变为两项之和：</p>
<p><code>loss=loss(y与y_)+REGULARIZER*loss(w)</code></p>
<p>其中，第一项是预测结果与标准答案之间的差距，如之前讲过的交叉熵、均方误差等；第二项是正则化计算结果。</p>
<ul>
<li>正则化计算方法：</li>
</ul>
<p>(1) <code>L1</code>正则化: <script type="math/tex">loss_{L1}=\sum_{i}|w_{i}|</script></p>
<p>用Tensorflow函数表示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w)</span><br></pre></td></tr></table></figure>
<p>(2) <code>L2</code>正则化: <script type="math/tex">loss_{L2}=\sum_{i}|w_{i}|^2</script><br>用Tensorflow函数表示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w)</span><br></pre></td></tr></table></figure>
<ul>
<li>用Tensorflow函数实现正则化：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.add_to_collection(&apos;losses&apos;,tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">loss = cem + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br></pre></td></tr></table></figure>
<p>其中<code>cem</code>见前面提到的。</p>
<p>例如：用300个符合正态分布的点<script type="math/tex">X[x_{0},x_{1}]</script>作为数据集，根据点<script type="math/tex">X[x_{0},x_{1}]</script>计算生成标注<code>Y_</code>，将数据集标注为红色点和蓝色点。标注规则为：当<script type="math/tex">x_{0}^2 + x_{1}^2 < 2</script>时，<code>y_=1</code>，标注为红色；当<script type="math/tex">x_{0}^2 + x_{1}^2 \geq 2</script>时，<code>y_=0</code>标注为蓝色。我们分别用无正则化和有正则化两种方法拟合曲线，把红色点和蓝色点分开。在实际分类时，如果前向传播输出的预测值<code>y</code>接近1则为红色点概率越大，接近0则为蓝色点概率越大，输出的预测值<code>y</code>为0.5是红蓝点概率分界线。</p>
<p>在本例子中，我们使用了之前未用过的模块与函数:</p>
<ul>
<li><p><code>matplotlib</code>模块：Python中的可视化工具模块，实现函数可视化，终端安装指令: <code>sudo pip install matplotlib</code></p>
</li>
<li><p>函数<code>plt.scatter()</code>:利用指定颜色实现点<code>(x,y)</code>的可视化</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x坐标, y坐标, c=&quot;颜色&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>本例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#0导入模块，生成模拟数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">BATCH_SIZE = <span class="number">30</span></span><br><span class="line">seed = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#基于seed产生随机数</span></span><br><span class="line">rdm = np.random.RandomState(seed)</span><br><span class="line"><span class="comment">#随机数返回300行2列的矩阵，表示300组坐标点(x0,x1)作为输入数据集</span></span><br><span class="line">X = rdm.randn(<span class="number">300</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#从X这个300行2列的矩阵中取出一行，判断如果两个坐标的平方和小于2，给Y赋值1，其余赋值0</span></span><br><span class="line"><span class="comment">#作为输入数据集的标签（正确答案）</span></span><br><span class="line">Y = [int(x0*x0+x1*x1 &lt; <span class="number">2</span>) <span class="keyword">for</span> (x0,x1) <span class="keyword">in</span> X]</span><br><span class="line"><span class="comment">#遍历Y中的每个元素，1赋值'red'其余赋值'blue'，这样可视化显示时人可以直观区分</span></span><br><span class="line">Y_c = [[<span class="string">'red'</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">'blue'</span>] <span class="keyword">for</span> y <span class="keyword">in</span> Y_]</span><br><span class="line"><span class="comment">#对数据集X和标签Y进行shape整理，第一个元素为-1表示，随第二个参数计算得到，第二个元素表示</span></span><br><span class="line"><span class="comment">#多少列，把X整理为n行2列，把Y整理为n行1列</span></span><br><span class="line">X = np.vstack(X).reshape(<span class="number">-1</span>,<span class="number">2</span>)</span><br><span class="line">Y_ = np.vstack(Y_).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">print(X)</span><br><span class="line">print(Y_)</span><br><span class="line">print(Y_c)</span><br><span class="line"><span class="comment">#用plt.scatter画出数据集X各行中第0列元素和第1列元素的点即各行的(x0,y0)，用各行Y_c对应的</span></span><br><span class="line"><span class="comment">#值表示颜色(c是color的缩写)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=np.squeeze(Y_c))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义神经网络的输入、参数和输出，定义前向传播过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape,regularizer)</span>:</span></span><br><span class="line">  w = tf.Variable(tf.random_normal(shape),dtype=tf.float32)</span><br><span class="line">  tf.add_to_collection(<span class="string">'losses'</span>,tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">  <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">  b = tf.Variable(tf.constant(<span class="number">0.01</span>,shape=shape))</span><br><span class="line">  <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = get_weight([<span class="number">2</span>,<span class="number">11</span>],<span class="number">0.01</span>)</span><br><span class="line">b1 = get_bias([<span class="number">11</span>])</span><br><span class="line">y1 = tf.nn.relu(tf.matmul(x,w1)+b1)</span><br><span class="line"></span><br><span class="line">w2 = get_weight([<span class="number">11</span>,<span class="number">1</span>],<span class="number">0.01</span>)</span><br><span class="line">b2 = get_bias([<span class="number">1</span>])</span><br><span class="line">y = tf.matmul(y1,w2)+b2 <span class="comment">#输出层不过激活</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">loss_total = loss_mse + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义反向传播方法：不含正则化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  STEPS = <span class="number">40000</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % <span class="number">300</span></span><br><span class="line">    end = start + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">      loss_mse_v = sess.run(loss_mse, feed_dict=&#123;x:X, y_:Y_&#125;)</span><br><span class="line">      print(<span class="string">"After %d steps, loss is: %f"</span> % (i,loss_mse_v))</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#xx在-3到3之间以步长0.01，yy在-3到3之间以步长0.01，生成二维网络坐标点</span></span><br><span class="line">  xx, yy = np.mgrid(<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>)</span><br><span class="line">  <span class="comment">#将xx，yy拉直，并合并成一个2列的矩阵，得到一个网络坐标点的集合</span></span><br><span class="line">  grid = np.c_[XX.ravel(),yy.ravel()]</span><br><span class="line">  <span class="comment">#将网络坐标点喂入神经网络，probs为输出</span></span><br><span class="line">  probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">  <span class="comment">#probs的shape调整成xx的样子</span></span><br><span class="line">  probs = probs.reshape(xx.shape)</span><br><span class="line">  print(<span class="string">"w1:\n"</span>, sess.run(w1))</span><br><span class="line">  print(<span class="string">"b1:\n"</span>, sess.run(b1))</span><br><span class="line">  print(<span class="string">"w2:\n"</span>, sess.run(w2))</span><br><span class="line">plt.scatter(X[:<span class="number">0</span>],X[:,<span class="number">1</span>],c=np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx,yy,probs,levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义反向传播方法: 包含正则化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_total)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  STEPS = <span class="number">40000</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">    start = (i*BATCH_SIZE) % <span class="number">300</span></span><br><span class="line">    end = start + BATCH_SIZE</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">      loss_v = sess.run(loss_total, feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line">      print(<span class="string">"After %d steps, loss is: %f"</span> %(i, loss_v))</span><br><span class="line">    </span><br><span class="line">    xx, yy = np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    print(<span class="string">"w1:\n"</span>, sess.run(w1))</span><br><span class="line">    print(<span class="string">"b1:\n"</span>, sess.run(b1))</span><br><span class="line">    print(<span class="string">"w2:\n"</span>, sess.run(w2))</span><br><span class="line">    print(<span class="string">"b2:\n"</span>, sess.run(b2))</span><br><span class="line">plt.scatter(X[:<span class="number">0</span>],X[:,<span class="number">1</span>],c=np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx,yy,probs,levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>执行代码，效果如下：</p>
<p>首先，数据集实现可视化，<script type="math/tex">x_{0}^2+x_{1}^2 < 2</script>的点显示红色，<script type="math/tex">x_{0}^2+x_{1}^2 \geq 2</script>的点显示蓝色，如图所示：</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328790917732.jpg" width="400"></p>
<p>接着，执行无正则化的训练过程，把红色的点和蓝色的点分开，生成曲线如下图所示：</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328792396453.jpg" width="400"></p>
<p>最后，执行有正则化的训练过程，把红色的点和蓝色的点分开，生成曲线如下图所示：</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328793451699.jpg" width="400"></p>
<p>对比无正则化与有正则化模型的训练结果，可看出有正则化模型的拟合曲线平滑，模型具有更好的泛化能力。</p>
<h2 id="搭建模块化神经网络"><a href="#搭建模块化神经网络" class="headerlink" title="搭建模块化神经网络"></a>搭建模块化神经网络</h2><ul>
<li>前向传播：由输入到输出，搭建完整的网络结构</li>
</ul>
<p>描述前向传播的过程需要定义三个函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def forward(x, regularizer):</span><br><span class="line">  w = </span><br><span class="line">  b = </span><br><span class="line">  y = </span><br><span class="line">  return y</span><br></pre></td></tr></table></figure>
<p>第一个函数<code>forward()</code>完成网络结构的设计，从输入到输出搭建完整的网络结构，实现前向传播过程。该函数中，参数<code>x</code>为输入，<code>regularizer</code>为正则化权重，返回值为预测或分类结果<code>y</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">  w = tf.Variable()</span><br><span class="line">  tf.add_to_collection(&apos;losses&apos;, tf.contrib.l2_regularizer(regularizer)(w))</span><br><span class="line">  return w</span><br></pre></td></tr></table></figure>
<p>第二个函数<code>get_weight()</code>对参数<code>w</code>设定。该函数中，参数<code>shape</code>表示参数<code>w</code>的形状，<code>regularizer</code>表示正则化权重，返回值为参数<code>w</code>。其中，<code>tf.Variable()</code>给<code>w</code>赋初值，<code>tf.add_to_collection()</code>表示将参数<code>w</code>正则化损失加到总损失<code>losses</code>中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def get_bias(shape):</span><br><span class="line">  b = tf.Variable()</span><br><span class="line">  return b</span><br></pre></td></tr></table></figure>
<p>第三个函数<code>get_bias()</code>对参数<code>b</code>进行设定。该函数中，参数<code>shape</code>表示参数<code>b</code>的形状，返回值为参数<code>b</code>。其中，<code>tf.Variable()</code>表示给<code>b</code>赋初值。</p>
<ul>
<li>反向传播：训练网络，优化网络参数，提高模型准确性。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def backward():</span><br><span class="line">  x = tf.placeholder()</span><br><span class="line">  y_ = tf.placeholder()</span><br><span class="line">  y = forward.forward(x, REGULARIZER)</span><br><span class="line">  global_step = tf.Varibale(0, trainable=False)</span><br><span class="line">  loss =</span><br></pre></td></tr></table></figure>
<p>函数<code>backward()</code>中，<code>placeholder()</code>实现对数据集<code>x</code>和标准答案<code>y_</code>占位，<code>forward.forward()</code>实现前向传播的网络结构，参数<code>global_step</code>表示训练轮数，设置为不可训练型参数。在训练网络模型时，常将正则化、指数衰减学习率和滑动平均这三个方法作为模型优化方法。</p>
<ul>
<li>在Tensorflow中，正则化表示为：</li>
</ul>
<p>首先，计算预测结果与标准答案的损失值</p>
<p>(1) <code>MSE</code>: <code>y</code>与<code>y_</code>的差距(loss<em>mse) = `tf.reduce_mean(tf.square(y-y</em>))<code>(2) 交叉熵:</code>ce = tf.nn.sparse<em>softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y</em>,1))<code>`y</code>与<code>y_</code>的差距(cem) = <code>tf.reduce_mean(ce)</code><br>(3) 自定义: <code>y</code>与<code>y_</code>的差距</p>
<p>其次，总损失值为预测结果与标准答案的损失值加上正则化项</p>
<p><code>loss = y 与 y_的差距 + tf.add_n(tf.get_collection(&#39;losses&#39;))</code></p>
<ul>
<li>在Tensorflow中，指数衰减学习率表示为:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    数据集总样本数 / BATCH_SIZE,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase=True)</span><br><span class="line">train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br></pre></td></tr></table></figure>
<ul>
<li>在 Tensorflow 中，滑动平均表示为:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">  train_op = tf.no_op(name=&apos;train&apos;)</span><br></pre></td></tr></table></figure>
<p>其中，滑动平均和指数衰减学习率中的 global_step 为同一个参数。</p>
<ul>
<li>用 with 结构初始化所有参数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">  init_op = tf.global_variables_initializer()</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  for i in range(STEPS):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x:, y_: &#125;)</span><br><span class="line">    if i % 轮数 == 0:</span><br><span class="line">      print</span><br></pre></td></tr></table></figure>
<p>其中，<code>with</code>结构用于初始化所有参数信息以及实现调用训练过程，并打印出<code>loss</code>值。</p>
<ul>
<li>判断 python 运行文件是否为主文件</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if __name__==&apos;__main__&apos;:</span><br><span class="line">  backward()</span><br></pre></td></tr></table></figure>
<p>该部分用来判断 python 运行的文件是否为主文件。若是主文件，则执行 backword()函数。</p>
<p>例如:用 300 个符合正态分布的点<script type="math/tex">X[x_{0}, x_{1}]</script>作为数据集，根据点<script type="math/tex">X[x_{0}, x_{1}]</script>的不同进行标注<code>Y_</code>，将数据集标注为红色和蓝色。标注规则为:当<script type="math/tex">x_{0} + x_{1} < 2</script>时，<code>y_=1</code>，点 <code>X</code>标注为红色;当<script type="math/tex">x_{0} + x_{1} \geq 2</script>时，<code>y_=0</code>，点<code>X</code>标注为蓝色。我们加入指数衰减学习率优化效率，加入正则化提高泛化性，并使用模块化设计方法，把红色点和蓝色点分开。</p>
<p>代码总共分为三个模块:生成数据集(generateds.py)、前向传播(forward.py)、反向传播 (backward.py)。</p>
<p>(1)生成数据集的模块(generateds.py)</p>
<p>(2)前向传播模块(forward.py)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#0导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入、参数和输出，定义前向传播过程</span><br><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">  w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">  tf.add_to_collection(&apos;losses&apos;,tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">  return w</span><br><span class="line">  </span><br><span class="line">def get_bias(shape):</span><br><span class="line">  b = tf.Variable(tf.constant(0.01, shape=shape))</span><br><span class="line">  return b</span><br><span class="line"></span><br><span class="line">def forward(x, regularizer):</span><br><span class="line">  w1 = get_weight([2,11], regularizer)</span><br><span class="line">  b1 = get_bias([11])</span><br><span class="line">  y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line">  </span><br><span class="line">  w2 = get_weight([11,1], regularizer)</span><br><span class="line">  b2 = get_bias([1])</span><br><span class="line">  y = tf.matmul(y1, w2) + b2 #输出层不过激活</span><br><span class="line">  </span><br><span class="line">  return y</span><br></pre></td></tr></table></figure>
<p>(3) 反向传播模块(backward.py)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#0导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import generateds</span><br><span class="line">import forward</span><br><span class="line"></span><br><span class="line">STEPS = 40000</span><br><span class="line">BATCH_SIZE = 30</span><br><span class="line">LEARNING_RATE_BASE = 0.001</span><br><span class="line">LEARNING_RATE_DECAY = 0.999</span><br><span class="line">REGULARIZER = 0.01</span><br><span class="line"></span><br><span class="line">def backward():</span><br><span class="line">  x = tf.placeholder(tf.float32, shape=(None,2))</span><br><span class="line">  y_ = tf.placeholder(tf.float32, shape=(None,1))</span><br><span class="line">  </span><br><span class="line">  X, Y_, Y_c = generateds.generateds()</span><br><span class="line">  y = forward.forward(x, REGULARIZER)</span><br><span class="line">  global_step = tf.Variable(0, trainable=False)</span><br><span class="line">  learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE, </span><br><span class="line">    global_step,</span><br><span class="line">    300/BATCH_SIZE,</span><br><span class="line">    LEARNING_RATE_DACAY,</span><br><span class="line">    staircase=True)</span><br><span class="line"></span><br><span class="line">  # 定义损失函数</span><br><span class="line">  loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">  loss_total = loss_mse + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line">  </span><br><span class="line">  # 定义反向传播方法：包含正则化</span><br><span class="line">  train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_total)</span><br><span class="line">  with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_varibales_initailizer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">      start = (i*BATCH_SIZE) % 300</span><br><span class="line">      end = start + BATCH_SIZE</span><br><span class="line">      sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">      if i % 2000 == 0:</span><br><span class="line">        loss_v = sess.run(loss_total, feed_dict=&#123;x:X,y_:Y&#125;)</span><br><span class="line">        print(&quot;After %d steps, loss is: %f&quot; % (i, loss_v))</span><br><span class="line">    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    </span><br><span class="line">  plt.scatter(X[:0], X[:,1], c=np.squeeze(Y_c))</span><br><span class="line">  plt.contour(xx, yy, probs, levels=[.5])</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">  backward()</span><br></pre></td></tr></table></figure>
<p>运行代码结果如下：</p>
<p><img src="http://ovhbzkbox.bkt.clouddn.com/2018-07-29-15328793451699.jpg" width="400"></p>
<p>由运行结果可见，程序使用模块化设计方法，加入指数衰减学习率，使用正则化后，红色点和蓝色点 的分割曲线相对平滑，效果变好。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/16/facenet/" rel="next" title="一种用于人脸识别和聚类的统一编码">
                <i class="fa fa-chevron-left"></i> 一种用于人脸识别和聚类的统一编码
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://oueonj87a.bkt.clouddn.com/WechatIMG3.jpeg"
               alt="Scott Du" />
          <p class="site-author-name" itemprop="name">Scott Du</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#搭建一个神经网络-总结搭建方法"><span class="nav-number">1.</span> <span class="nav-text">搭建一个神经网络,总结搭建方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#搭建模块化神经网络"><span class="nav-number">2.</span> <span class="nav-text">搭建模块化神经网络</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Scott Du</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>
